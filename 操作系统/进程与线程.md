# 进程与线程

---
## task_struct

- **进程状态**
1. 5个互斥状态：

状态 | 描述
-----|------
TASK_RUNNING | 表示进程要么正在执行，要么正要准备执行（已经就绪），正在等待cpu时间片的调度
TASK_INTERRUPTIBLE | 进程因为等待一些条件而被挂起（阻塞）而所处的状态，这些条件主要包括：硬中断、资源、一些信号……，一旦等待的条件成立，进程就会从该状态（阻塞）迅速转化成为就绪状态TASK_RUNNING
TASK_UNINTERRUPTIBLE | 意义与TASK_INTERRUPTIBLE类似，除了不能通过接受一个信号来唤醒以外，对于处于TASK_UNINTERRUPIBLE状态的进程，哪怕我们传递一个信号或者有一个外部中断都不能唤醒他们，只有它所等待的资源可用的时候，他才会被唤醒，这个标志很少用，但是并不代表没有任何用处，其实他的作用非常大，特别是对于驱动刺探相关的硬件过程很重要，这个刺探过程不能被一些其他的东西给中断，否则就会让进程进入不可预测的状态
TASK_STOPPED | 进程被停止执行，当进程接收到SIGSTOP、SIGTTIN、SIGTSTP或者SIGTTOU信号之后就会进入该状态
TASK_TRACED | 表示进程被debugger等进程监视，进程执行被调试程序所停止，当一个进程被另外的进程所监视，每一个信号都会让进城进入该状态

2. 2个终止状态：

状态 | 描述
-----|-----
EXIT_ZOMBIE | 进程的执行被终止，但是其父进程还没有使用wait()等系统调用来获知它的终止信息，此时进程成为僵尸进程
EXIT_DEAD | 进程的最终状态

3. 内核如何将进程置为睡眠状态：将进程状态设置为 TASK_INTERRUPTIBLE 或 TASK_UNINTERRUPTIBLE 并调用调度程序的 schedule() 函数，这样会将进程从 CPU 运行队列中移除   
(1)如果进程处于可中断模式的睡眠状态（TASK_INTERRUPTIBLE），可以通过显式的唤醒呼叫（wakeup_process()）或需要处理的信号来唤醒它当处于可中断睡眠模式的任务接收到信号时，它需要处理该信号（除非它已被屏弊），离开之前正在处理的任务（此处需要清除代码），并将 -EINTR 返回给用户空间，检查这些返回代码和采取适当操作的工作将由程序员完成   
(2)如果进程处于非可中断模式的睡眠状态（TASK_UNINTERRUPTIBLE），只能通过显式的唤醒呼叫将其唤醒，对不可中断睡眠模式的进程的唤醒呼叫可能会由于某些原因不会发生，这会使进程无法被终止，从而会生成永远不会停止的进程，惟一的解决方法就是重启系统   
(3)新的进程睡眠状态（TASK_KILLABLE），运行原理类似于 TASK_UNINTERRUPTIBLE，但是可以响应致命信号

- **进程标识符（PID）**

```
pid_t pid;  
pid_t tgid;  
```
1. Unix系统通过pid来标识进程，linux把不同的pid与系统中每个进程或轻量级线程关联，而unix程序员希望同一组线程具有共同的pid，遵照这个标准linux引入线程组的概念，一个线程组所有线程与领头线程具有相同的pid，存入tgid字段，只有线程组的领头线程的pid成员才会被设置为与tgid相同的值，getpid()返回当前进程的tgid值而不是pid的值
2. 在CONFIG_BASE_SMALL配置为0的情况下，PID的取值范围是0到32767，即系统中的进程数最大为2^15 = 32768个

- **进程内核栈**
1. 对每个进程，Linux内核都把两个不同的数据结构紧凑的存放在一个单独为进程分配的内存区域中：一个是内核态的进程堆栈，另一个是紧挨着进程描述符的小数据结构thread_info，叫做线程描述符
2. Linux把thread_info（线程描述符）和内核态的线程堆栈存放在一起，这块区域通常是8K（占两个页框），其实地址必须是8192的整数倍
3. 内核态的进程访问处于内核数据段的栈，这个栈不同于用户态的进程所用的栈，用户态进程所用的栈，是在进程线性地址空间中，而内核栈是当进程从用户空间进入内核空间时，特权级发生变化，需要切换堆栈，那么内核空间中使用的就是这个内核栈，因为内核控制路径使用很少的栈空间，所以只需要几千个字节的内核态堆栈，并且内核态堆栈仅用于内核例程，Linux内核另外为中断提供了单独的硬中断栈和软中断栈

```
// Linux内核中使用一个联合体来表示一个进程的线程描述符和内核栈
union thread_union
{
    struct thread_info thread_info;
    unsigned long stack[THREAD_SIZE/sizeof(long)];
};
```
5. 进程通过 alloc_thread_info_node 函数分配它的内核栈，通过 free_thread_info 函数释放所分配的内核栈

- **进程标记**
1. 反应进程状态的信息，但不是运行状态，用于内核识别进程当前的状态，以备下一步操作

```
// flags成员的可能取值如下，这些宏以PF(ProcessFlag)开头

unsigned int flags; /* per process flags, defined below */ 

PF_FORKNOEXEC 进程刚创建，但还没执行
PF_SUPERPRIV 超级用户特权
PF_DUMPCORE dumped core
PF_SIGNALED 进程被信号(signal)杀死
PF_EXITING 进程开始关闭
```

- **表示进程亲属关系的成员**

字段 | 描述
-----|---
real_parent | 指向其父进程，如果创建它的父进程不再存在，则指向PID为1的init进程
parent | 指向其父进程，当它终止时，必须向它的父进程发送信号，它的值通常与real_parent相同
children | 表示链表的头部，链表中的所有元素都是它的子进程
sibling | 用于把当前进程插入到兄弟链表中
group_leader | 指向其所在进程组的领头进程

- **进程调度**
1. 优先级：

字段 | 描述
-----|---
static_prio | 用于保存静态优先级，可以通过nice系统调用来进行修改，普通进程的静态优先级范围是从MAX_RT_PRIO到MAX_PRIO-1（即100到139），值越大静态优先级越低
rt_priority | 用于保存实时优先级，范围是0到MAX_RT_PRIO-1（即99）
normal_prio | 值取决于静态优先级和调度策略
prio | 用于保存动态优先级

2. 调度策略相关字段：
字段 | 描述
-----|---
policy | 调度策略
sched_class | 调度类
se | 普通进程的调用实体，每个进程都有其中之一的实体
rt | 实时进程的调用实体，每个进程都有其中之一的实体
cpus_allowed | 用于控制进程可以在哪里处理器上运行
3. 五种调度策略：

字段 | 描述 | 所在调度器类
-----|------|-------------
SCHED_NORMAL | 用于普通进程，通过CFS调度器实现，SCHED_BATCH用于非交互的处理器消耗型进程，SCHED_IDLE是在系统负载很低时使用 | CFS
SCHED_BATCH | SCHED_NORMAL普通进程策略的分化版本，采用分时策略，根据动态优先级(可用nice()API设置），分配 CPU 运算资源，在有实时进程存在时，实时进程优先调度 | CFS
SCHED_IDLE | 优先级最低，在系统空闲时才跑这类进程 | CFS
SCHED_FIFO | 先入先出调度算法（实时调度策略），相同优先级的任务先到先服务，高优先级的任务可以抢占低优先级的任务 | RT
SCHED_RR | 轮流调度算法（实时调度策略），后者提供 Roound-Robin 语义，采用时间片，相同优先级的任务当用完时间片会被放到队列尾部，以保证公平性，同样，高优先级的任务可以抢占低优先级的任务，不同要求的实时任务可以根据需要用 sched_setscheduler()API 设置策略 | RT

4. 四种调度类：

字段 | 描述
-----|---
idle_sched_class | 每个cpu的第一个pid=0线程：swapper，是一个静态线程，调度类属于idel_sched_class，所以在ps里面是看不到的，一般运行在开机过程和cpu异常的时候做dump
stop_sched_class | 优先级最高的线程，会中断所有其他线程，且不会被其他任务打断，作用：1. 发生在cpu_stop_cpu_callback 进行cpu之间任务migration，2.HOTPLUG_CPU的情况下关闭任务
rt_sched_class| RT，作用：实时线程
fair_sched_class | CFS（公平），作用：一般常规线程

目前系統中,Scheduling Class的优先级顺序为StopTask > RealTime > Fair > IdleTask，开发者可以根据己的设计需求，來把所属的Task配置到不同的Scheduling Class中

- **进程地址空间**

字段 | 描述
-----|---
mm | 进程所拥有的用户空间内存描述符，内核线程无的mm为NULL
active_mm | active_mm指向进程运行时所使用的内存描述符， 对于普通进程而言，这两个指针变量的值相同，但是内核线程kernel thread是没有进程地址空间的，所以内核线程的tsk->mm域是空（NULL），但是内核必须知道用户空间包含了什么，因此它的active_mm成员被初始化为前一个运行进程的active_mm值
brk_randomized | 用来确定对随机堆内存的探测
rss_stat | 用来记录缓冲信息

- **判断标志**

字段 | 描述
-----|---
exit_code | 用于设置进程的终止代号，这个值要么是_exit()或exit_group()系统调用参数（正常终止），要么是由内核提供的一个错误代号（异常终止）
exit_signal | 被置为-1时表示是某个线程组中的一员，只有当线程组的最后一个成员终止时，才会产生一个信号，以通知线程组的领头进程的父进程
pdeath_signal | 用于判断父进程终止时发送信号
personality | 用于处理不同的ABI
in_execve | 用于通知LSM是否被do_execve()函数所调用
in_iowait | 用于判断是否进行iowait计数
sched_reset_on_fork | 用于判断是否恢复默认的优先级或调度策略

- **信号处理**

字段 | 描述
-----|---
signal | 指向进程的信号描述符
sighand | 指向进程的信号处理程序描述符
blocked | 表示被阻塞信号的掩码，real_blocked表示临时掩码
pending | 存放私有挂起信号的数据结构
sas_ss_sp | 是信号处理程序备用堆栈的地址，sas_ss_size表示堆栈的大小

- **用于保护资源分配或释放的自旋锁**

```
spinlock_t alloc_lock;
```

- **进程描述符使用计数**

```
// 被置为2时，表示进程描述符正在被使用而且其相应的进程处于活动状态
atomic_t usage;
```

- **do_fork函数**

```
// 在执行do_fork()时，如果给定特别标志，则vfork_done会指向一个特殊地址
// 如果copy_process函数的clone_flags参数的值被置为CLONE_CHILD_SETTID或CLONE_CHILD_CLEARTID，则会把child_tidptr参数的值分别复制到set_child_tid和clear_child_tid成员，这些标志说明必须改变子进程用户态地址空间的child_tidptr所指向的变量的值

struct completion *vfork_done;      /* for vfork() */  
int __user *set_child_tid;          /* CLONE_CHILD_SETTID */ 
int __user *clear_child_tid;        /* CLONE_CHILD_CLEARTID */ 
```

- **命名空间**

```
struct nsproxy *nsproxy; 
```

- **Control Groups**

```
#ifdef CONFIG_CGROUPS  
    /* Control Group info protected by css_set_lock */  
    struct css_set __rcu *cgroups;  
    /* cg_list protected by css_set_lock and tsk->alloc_lock */  
    struct list_head cg_list;  
#endif  
#ifdef CONFIG_CGROUP_MEM_RES_CTLR /* memcg uses this to do batch job */  
    struct memcg_batch_info {  
        int do_batch;   /* incremented when batch uncharge started */  
        struct mem_cgroup *memcg; /* target memcg of uncharge */  
        unsigned long bytes;        /* uncharged usage */  
        unsigned long memsw_bytes; /* uncharged mem+swap usage */  
    } memcg_batch;  
#endif
```
- **futex同步机制**

```
#ifdef CONFIG_FUTEX  
    struct robust_list_head __user *robust_list;  
#ifdef CONFIG_COMPAT  
    struct compat_robust_list_head __user *compat_robust_list;  
#endif  
    struct list_head pi_state_list;  
    struct futex_pi_state *pi_state_cache;  
#endif
```

- **管道**

```
struct pipe_inode_info *splice_pipe;
```

- **time slack values，常用于poll和select函数 **

```
unsigned long timer_slack_ns;  
unsigned long default_timer_slack_ns;
```

- **socket控制消息（control message）**

```
struct list_head    *scm_work_list;
```

> [Linux进程描述符task_struct结构体详解--Linux进程的管理与调度（一）](https://blog.csdn.net/gatieme/article/details/51383272)
---
## Linux的命名空间

- **Linux内核命名空间描述**

```
// 在该结构体中定义了5个指向各个类型namespace的指针，由于多个进程可以使用同一个namespace，所以nsproxy可以共享使用，count字段是该结构的引用计数
struct nsproxy
{
         atomic_t count;
         struct uts_namespace   *uts_ns;
         struct ipc_namespace   *ipc_ns;
         struct mnt_namespace   *mnt_ns;
         struct pid_namespace   *v;
         struct net             *net_ns;
};

// 系统中有一个默认的nsproxy，init_nsproxy，该结构在task初始化是也会被初始
struct nsproxy init_nsproxy = {
         .count                         = ATOMIC_INIT(1),
         .uts_ns                       = &init_uts_ns,
#if defined(CONFIG_POSIX_MQUEUE) || defined(CONFIG_SYSVIPC)
         .ipc_ns                        = &init_ipc_ns,
#endif
         .mnt_ns                      = NULL,
         .pid_ns_for_children        = &init_pid_ns,
#ifdef CONFIG_NET
         .net_ns                       = &init_net,
#endif
};
```
- **命名空间的创建**
1. 在用fork或clone系统调用创建新进程时，有特定的选项可以控制是与父进程共享命名空间，还是建立新的命名空间
2. unshare系统调用将进程的某些部分从父进程分离，其中也包括命名空间

- **PID Namespace**
1. 一个PID Namespace为进程提供了一个独立的PID环境，PID Namespace内的PID将从1开始，在Namespace内调用fork，vfork或clone都将产生一个在该Namespace内独立的PID，新创建的Namespace里的第一个进程在该Namespace内的PID将为1
2. 该Namespace内的孤儿进程都将以该进程为父进程，当该进程被结束时，该Namespace内所有的进程都会被结束
3. PID Namespace是层次性，新创建的Namespace将会是创建该Namespace的进程属于的Namespace的子Namespace，子Namespace中的进程对于父Namespace是可见的，一个进程将拥有不止一个PID，而是在所在的Namespace以及所有直系祖先Namespace中都将有一个PID
4. 系统启动时，内核将创建一个默认的PID Namespace，该Namespace是所有以后创建的Namespace的祖先，因此系统所有的进程在该Namespace都是可见的

- **IPC Namespace**
1. 一个IPC Namespace有一组System V IPC objects 标识符构成，这标识符由IPC相关的系统调用创建，在一个IPC Namespace里面创建的IPC object对该Namespace内的所有进程可见，但是对其他Namespace不可见，这样就使得不同Namespace之间的进程不能直接通信，当一个IPC Namespace被销毁，该Namespace内的所有IPC object会被内核自动销毁
2. PID Namespace和IPC Namespace可以组合起来一起使用，创建的Namespace既是一个独立的PID空间又是一个独立的IPC空间，不同Namespace的进程彼此不可见，也不能互相通信，这样就实现了进程间的隔离

- **mount Namespace**
1. mount Namespace为进程提供了一个文件层次视图，如果不设定这个flag，子进程和父进程将共享一个mount Namespace，其后子进程调用mount或umount将会影响到所有该Namespace内的进程，如果子进程在一个独立的mount Namespace里面，就可以调用mount或umount建立一份新的文件层次视图

- **Network Namespace**
1. 一个Network Namespace为进程提供了一个完全独立的网络协议栈的视图，包括网络设备接口，IPv4和IPv6协议栈，IP路由表，防火墙规则，sockets等等
2. 一个物理设备只能存在于一个Network Namespace中，可以从一个Namespace移动另一个Namespace中
3. 利用虚拟化网络设备，可以建立到其他Namespace中的物理设备的桥接

- **UTS Namespace**
1. 一个UTS Namespace就是一组被uname返回的标识符，新的UTS Namespace中的标识符通过复制调用进程所属的Namespace的标识符来初始化，Clone出来的进程可以通过相关系统调用改变这些标识符，比如调用sethostname来改变该Namespace的hostname，这一改变对该Namespace内的所有进程可见
2. CLONE_NEWUTS和CLONE_NEWNET一起使用，可以虚拟出一个有独立主机名和网络空间的环境，就跟网络上一台独立的主机一样

- **USER Namespace**
1. user namespace可以嵌套（目前内核控制最多32层），除了系统默认的user namespace外，所有的user namespace都有一个父user namespace，每个user namespace都可以有零到多个子user namespace
2. 在不同的user namespace中，同样一个用户的user ID 和group ID可以不一样，即一个用户可以在父user namespace中是普通用户，在子user namespace中是超级用户（超级用户只相对于子user namespace所拥有的资源，无法访问其他user namespace中需要超级用户才能访问资源）
3. 要把容器中的uid和真实系统的uid给映射在一起（容器内是 root，被映射到容器外一个非 root 用户），需要修改 /proc/<pid>/uid_map 和 /proc/<pid>/gid_map 这两个文件，文件的格式为： ID-inside-ns ID-outside-ns length

> [Linux的命名空间详解--Linux进程的管理与调度（二）](https://blog.csdn.net/gatieme/article/details/51383322)
---
## Linux进程ID号

- **进程ID类型**
1. PID：Linux 内核唯一区分每个进程的标识，在使用 fork 或 clone 系统调用时产生的进程均会由内核分配一个新的唯一的PID值
2. TGID：线程组（轻量级进程组）的ID标识，在一个进程中，如果以CLONE_THREAD标志来调用clone建立的进程就是该进程的一个线程（即轻量级进程，Linux其实没有严格的线程概念），它们处于一个线程组，该线程组的所有线程的ID叫做TGID   
(1)处于相同的线程组中的所有线程都有相同的TGID，但是由于他们是不同的线程，因此其pid各不相同   
(2)只有线程组组长（也叫主线程）的TGID与其PID相同   
(3)一个进程没有使用线程，则其TGID与PID也相同   
(4)getpid()系统调用返回的是当前进程的tgid值而不是pid值
3. PGID：独立的进程可以组成进程组（使用setpgrp系统调用），进程组可以简化向所有组内进程发送信号的操作，例如用管道连接的进程处在同一进程组内（管道的原理就是在创建2个子进程）
4. SID：几个进程组可以合并成一个会话组（使用setsid系统调用），可以用于终端程序设计，会话组中所有进程都有相同的SID，保存在task_struct的session成员中

- **PID命名空间**
1. 局部ID和全局ID：   
(1)全局ID 在内核本身和初始命名空间中唯一的ID，在系统启动期间开始的 init 进程即属于该初始命名空间，系统中每个进程都对应了该命名空间的一个PID，叫全局ID，保证在整个系统中唯一   
(2)局部ID 对于属于某个特定的命名空间，它在其命名空间内分配的ID为局部ID，该ID也可以出现在其他的命名空间中

- **pid_namespace**

字段 | 描述
-----|-----
kref | 表示指向pid_namespace的个数
pid_cachep | 域指向分配pid的slab的地址
pidmap | pidmap结构体表示分配pid的位图
last_pid | 用于pidmap的分配
child_reaper | 指向的是当前命名空间的init进程，每个命名空间都有一个作用相当于全局init进程的进程
level | 代表当前命名空间的等级，初始命名空间的level为0，它的子命名空间level为1，依次递增，而且子命名空间对父命名空间是可见的
parent | 指向父命名空间的指针

- **pidmap 当需要分配一个新的pid时查找可使用pid的位图**

字段 | 描述
-----|-----
nr_free | 表示还能分配的pid的数量
page | 指向的是存放pid的物理页

- **pid**

字段 | 描述
-----|-----
count | 指使用该PID的task的数目
level | 表示可以看到该PID的命名空间的数目，也就是包含该进程的命名空间的深度
tasks[PIDTYPE_MAX] | 是一个数组，每个数组项都是一个散列表头,分别对应PIDTYPE_PID、 PIDTYPE_PGID、PIDTYPE_SID三种类型，它将属于该类型的进程task_struct 中 pid_link.node链接起来（例如进程A是进程B和C的组长，tasks[PIDTYPE_PGID]将所有以A进程的pid为组长的进程链接起来）
numbers[1] | 一个upid的实例数组，每个数组项代表一个命名空间，用来表示一个PID可以属于不同的命名空间，该元素放在末尾，可以向数组添加附加的

- **upid 特定的命名空间中可见的信息**

字段 | 描述
-----|-----
nr | 表示ID具体的值
ns | 指向命名空间的指针
pid_chain | 所有的upid实例都保存在一个散列表中，链地址解决散列表hash冲突

- **task_struct**

字段 | 描述
-----|-----
pid | 指该进程的进程描述符，在fork函数中对其进行赋值的
tgid | 指该进程的线程描述符，在linux内核中对线程并没有做特殊的处理，还是由task_struct来管理，所以从内核的角度看， 用户态的线程本质上还是一个进程，对于同一个进程（用户态角度）中不同的线程其tgid是相同的，但是pid各不相同，主线程即group_leader（主线程会创建其他所有的子线程），如果是单线程进程（用户态角度），它的pid等于tgid
group_leader | 除了在多线程的模式下指向主线程，还有一个用处， 当一些进程组成一个群组时（PIDTYPE_PGID)， 该域指向该群组的leader
pids | pid_link三种类型 PIDTYPE_PID、PIDTYPE_PGID、PIDTYPE_SID
nsproxy | 指针指向namespace相关的域，通过nsproxy域可以知道该task_struct属于哪个pid_namespace

- **pid_link**

字段 | 描述
-----|-----
node | pid.tasks[PIDTYPE_MAX]组织的进程链接表
pid | PIDTYPE_PID、PIDTYPE_PGID、PIDTYPE_SID三种类型类型pid指针

- **内核设计的要素**
1. 快速地根据进程的 task_struct、ID类型、命名空间找到局部ID
2. 快速地根据局部ID、命名空间、ID类型找到对应进程的 task_struct
3. 快速地给新进程在可见的命名空间内分配一个唯一的 PID

> [Linux进程ID号--Linux进程的管理与调度（三）](https://blog.csdn.net/gatieme/article/details/51383377)


---
## Linux 线程实现机制分析

- **线程模型**
1. 针对线程模型的两大意义，分别开发出了核心级线程和用户级线程两种线程模型，分类的标准主要是线程的调度者在核内还是在核外
2. 核心级线程更利于并发使用多处理器的资源，用户级线程则更多考虑的是上下文切换开销

- **一对一模型**
1. 用一个线程（也许是轻量进程）对应一个核心线程，将线程调度交给核心完成
2. 线程的建立、析构以及同步都需要系统调用，系统调用的代价相对较高，需要在用户态和内核态中切换，每个线程都需要有一个内核线程支持，因此需要消耗内核线程的栈空间

- **多对一模型**
1. 在用户态实现多线程，调度也在用户态完成，用户线程的建立，同步，销毁，调度完全在用户空间完成，不需要内核的帮助，因此这种线程的操作是极其快速的且低消耗的
2. 内核并不知道用户线程的存在，用户线程之间的调度由在用户空间实现的线程库实现，一个用户线程如果阻塞在系统调用中，则整个进程都将会阻塞

- **多对多模型**
1. 在用户态实现多线程，调度也在用户态完成，用户线程的建立，同步，销毁，调度完全在用户空间完成（go协程）
2. 内核多个核心线程，用户线程与核心线程多对多关系，内核调度到内核线程执行的时候，关联的用户线程就被执行

- **Linux 2.4内核中的轻量进程实现**
1. 轻量进程：允许某些进程共享一部分资源，例如文件、信号，数据内存，甚至代码
2. 应用程序可以通过一个统一的clone()系统调用接口，用不同的参数指定创建轻量进程还是普通进程，clone()会调用do_fork()创建轻量进程或者普通进程，使用（CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND）参数来调用clone()创建"线程"，表示共享内存、共享文件系统访问计数、共享文件描述符表，以及共享信号处理方式

- **POSIX线程基本要求**
1. 查看进程列表的时候，相关的一组task_struct应当被展现为列表中的一个节点
2. 发送给这个"进程"的信号(对应kill系统调用)，将被对应的这一组task_struct所共享，并且被其中的任意一个"线程"处理
3. 发送给某个"线程"的信号(对应pthread_kill)，将只被对应的一个task_struct接收，并且由它自己来处理
4. 当"进程"被停止或继续时(对应SIGSTOP/SIGCONT信号)，对应的这一组task_struct状态将改变
5. 当"进程"收到一个致命信号(比如由于段错误收到SIGSEGV信号)，对应的这一组task_struct将全部退出
6. 以上可能不全

- **LinuxThread的线程机制**
1. 在linux 2.6以前，pthread线程库对应的实现是一个名叫 Linuxthreads 的lib，LinuxThread 基于核心轻量级进程的"一对一"线程模型，一个线程实体对应一个核心轻量级进程（进程有不同pid），而线程之间的管理在核外函数库中实现，但是对于POSIX提出的那些要求，Linuxthreads 除了第5点以外都没有实现
2. LinuxThreads定义了一个struct _pthread_descr_struct数据结构来描述线程，并使用全局数组变量__pthread_handles来保存创建的线程
3. LinuxThreads对线程库的实现进行了一些范围限制，每线程的私有数据key数：PTHREAD_KEYS_MAX = 1024，每进程的线程数：PTHREAD_THREADS_MAX = 1024，线程运行栈最小空间大小：PTHREAD_STACK_MIN = 16384
4. LinuxThreads 为了实现第5点创建管理线程来实现的（为什么A程序创建了10个线程，但是ps时却会出现11个A进程了），当程序开始运行时，并没有管理线程存在，程序第一次调用pthread_create时，Linuxthreads发现管理线程不存在，于是创建这个管理线程，这个管理线程是进程中的第一个线程(主线程)的儿子
5. 然后在pthread_create中，会通过pipe向管理线程发送一个命令，告诉它创建线程，即除主线程外所有的线程都是由管理线程来创建的，管理线程是它们的父亲，因此当任何一个子线程退出时，管理线程将收到SIGUSER1信号(这是在通过clone创建子线程时指定的)，管理线程在对应的sig_handler中会判断子线程是否正常退出，如果不是，则杀死所有线程，然后自杀
6. 主线程怎么办呢？主线程是管理线程的父亲，其退出时并不会给管理线程发信号，于是在管理线程的主循环中通过getppid检查父进程的ID号，如果ID号是1，说明父亲已经退出，并把自己托管给了init进程(1号进程)，这时候管理线程也会杀掉所有子线程，然后自杀，那么如果主线程是调用pthread_exit主动退出的呢？按照posix的标准，这种情况下其他子线程是应该继续运行的，于是在Linuxthreads中，主线程调用pthread_exit以后并不会真正退出，而是会在pthread_exit函数中阻塞等待所有子线程都退出了，pthread_exit才会让主线程退出(在这个等等过程中，主线程一直处于睡眠状态)
7. LinuxThread使用信号来模拟同步互斥
8. LinuxThread的信号处理的行为可以说跟POSIX的标准是完全不一致的，当你对一个进程发送一个信号后，只有拥有这个进程号的进程才有反应，而属于这个进程的线程因为拥有不同的进程号而无法做出响应
9. 瓶颈：线程的创建与销毁都是通过管理线程来完成的，于是管理线程就成了Linuxthreads的一个性能瓶颈，创建与销毁需要一次进程间通信，切换内核用户态造成性能降低，一次上下文切换之后才能被管理线程执行，并且多个请求会被管理线程串行地执行

- **NPTL(Native POSIX Threading Library)**
1. 在linux 2.6中，内核有了线程组的概念，task_struct结构中增加了一个tgid(thread group id)字段，如果这个task是一个"主线程"，则它的tgid等于pid，否则tgid等于进程的pid(即主线程的pid)，此外每个线程有自己的pid，在clone系统调用中，传递CLONE_THREAD参数就可以把新进程的tgid设置为父进程的tgid
2. 有了tgid，内核或相关的shell程序就知道某个tast_struct是代表一个进程还是代表一个线程，getpid(获取进程ID)系统调用返回的也是tast_struct中的tgid
3. 为了应付"发送给进程的信号"和"发送给线程的信号"，task_struct里面维护了两套signal_pending，一套是线程组共享的，一套是线程独有的，通过kill发送的信号被放在线程组共享的signal_pending中，可以由任意一个线程来处理，通过pthread_kill发送的信号(pthread_kill是pthread库的接口，对应的系统调用中tkill)被放在线程独有的signal_pending中，只能由本线程来处理

> [linux线程的实现](hhttps://www.cnblogs.com/zhaoyl/p/3620204.html)   
> [Linux 线程实现机制分析](https://www.ibm.com/developerworks/cn/linux/kernel/l-thread/index.html)   
> [LinuxThread vs NPTL](https://blog.csdn.net/ixidof/article/details/24579879)   
> [Linux线程的实现 & LinuxThread vs. NPTL & 用户级内核级线程 & 线程与信号处理](https://www.cnblogs.com/charlesblc/p/6242518.html)

---
## Linux下的进程类别（内核线程、轻量级进程和用户进程）以及其创建方式

- **Linux进程类别**
1. 一个进程由于其运行空间的不同，从而有==内核线程==和==用户进程==的区分，内核线程运行在内核空间，之所以称之为线程是因为它没有虚拟地址空间，只能访问内核的代码和数据，而用户进程则运行在用户空间，但是可以通过中断，系统调用等方式从用户态陷入内核态，但是内核态只是进程的一种状态，与内核线程有本质区别
2. 用户进程运行在用户空间上，而一些通过共享资源实现的一组进程我们称之为线程组，Linux下内核其实本质上没有线程的概念，Linux下线程其实上是与其他进程共享某些资源的进程而已，但是我们习惯上还是称他们为==线程==或者==轻量级进程==
3. 因此，Linux上进程分3，内核线程（或者叫核心进程）、用户进程、用户线程，当然如果更严谨的，你也可以认为用户进程和用户线程都是用户进程

- **进程与线程**
1. 进程是一个具有独立功能的程序关于某个数据集合的一次运行活动，它可以申请和拥有系统资源，是一个动态的概念，是一个活动的实体，它不只是程序的代码，还包括当前的活动，通过程序计数器的值和处理寄存器的内容来表示，进程是一个"执行中的程序"，程序是一个没有生命的实体，只有处理器赋予程序生命时，它才能成为一个活动的实体，我们称其为进程
2. 通常在一个进程中可以包含若干个线程，它们可以利用进程所拥有的资源，在引入线程的操作系统中，通常都是把进程作为分配资源的基本单位，而把线程作为独立运行和独立调度的基本单位，由于线程比进程更小，基本上不拥有系统资源，故对它的调度所付出的开销就会小得多，能更高效的提高系统内多个程序间并发执行的程度
3. 进程的个体间是完全独立的，而线程间是彼此依存的，多进程环境中，任何一个进程的终止，不会影响到其他进程，而多线程环境中，父线程终止，全部子线程被迫终止(没有了资源)，而任何一个子线程终止一般不会影响其他线程，除非子线程执行了exit()系统调用，任何一个子线程执行exit()，全部线程同时灭亡
4. 进程与线程区别：   
(1)地址空间和其它资源：进程间相互独立，同一进程的各线程间共享数据空间   
(2)通信：进程间通信IPC，线程间可以直接读写进程数据段（如全局变量）来进行通信，但需要同步和互斥手段的辅助，以保证数据的一致性   
(3)调度和切换：线程只拥有一点在运行中必不可少的资源（如程序计数器，一组寄存器和栈），因此线程上下文切换比进程上下文切换要快得多

- **内核线程**
1. 内核线程只运行在内核态，不受用户态上下文的拖累
2. 处理器竞争：可以在全系统范围内竞争处理器资源
3. 使用资源：唯一使用的资源是内核栈（内核栈跟task_struct的thread_info共享8k的空间）和上下文切换时保持寄存器的空间，内核线程没有自己的地址空间，所以它们的"current->mm"都是空的
4. 调度：调度的开销可能和进程自身差不多昂贵
5. 同步效率：资源的同步和数据共享比整个进程的数据同步和共享要低一些

- **进程的复制fork和加载execve**
1. fork生成当前进程的的一个相同副本，该副本成为子进程：该系统调用之后，原来的进程就有了两个独立的实例，这两个实例的联系包括：同一组打开文件，同样的工作目录，进程虚拟空间（内存）中同样的数据（当然两个进程各有一份副本，也就是说他们的虚拟地址相同，但是所对应的物理地址不同）等等
2. execve从一个可执行的二进制程序镜像加载应用程序，来代替当前运行的进程

- **写时复制技术**
1. 父进程和子进程共享页帧而不是复制页帧，父进程和子进程对这些页帧的访问权限设成只读，这些页帧被共享，就不能被修改，即页帧被保护
2. 无论父进程还是子进程何时试图写一个共享的页帧，就产生一个异常(page_fault int14)中断，这时内核do_wp_page()就把这个页复制到一个新的页帧中并标记为可写，父进程和子进程各自拥有一块内容相同的物理页面
3. 原来的页帧仍然是写保护的，当其他进程试图写入时，内核检查写进程是否是这个页帧的唯一属主，如果是，就把这个页帧标记为对这个进程是可写的

- **Linux下线程的实现机制**
1. 从内核的角度来说，他并没有线程这个概念，Linux把所有的进程都当做进程来实现，内核中并没有准备特别的调度算法或者定义特别的数据结构来表示线程，相反，线程仅仅被视为一个与其他进程共享某些资源的进程，每个线程都拥有唯一隶属于自己的task_struct，所以在内核看来，它看起来就像式一个普通的进程(只是线程和同组的其他进程共享某些资源)
2. 进程task_struct中pid存储的是内核对该进程的唯一标示，即对进程则标示进程号，对线程来说就是其线程号，对于线程来说一个线程组所有线程与领头线程具有相同的进程号，存入tgid字段，用户态下getpid()调用获取的是线程组tgid字段

- **提供专门线程支持的系统和Linux线程实现机制区别**
1. 在提供专门线程支持的系统中（Microsoft Windows 或者 Sun Solaris），通常会有一个包含只想四个不同线程的指针的进程描述符，该描述符复制描述像地址空间，打开的文件这样的共享资源，线程本身再去描述它独占的资源
2. Linux仅仅创建了四个进程，并分配四个普通的task_struct结构，然后建立这四个进程时制定他们共享某些资源

- **用户进程、用户线程、内核线程拥有资源**
1. 用户进程拥有 进程描述符、PID、进程正文段、核心堆栈 、用户空间的数据段和堆栈
2. 用户线程拥有 进程描述符、PID、进程正文段、核心堆栈，共享用户空间的数据段和堆栈，用户线程也可以通过exec函数族拥有自己的用户空间的数据段和堆栈，成为用户进程
3. 内核线程拥有 进程描述符、PID、进程正文段、核心堆栈

- **内核线程创建**
1. kernel_thread接口，使用该接口创建的线程，必须在该线程中调用daemonize()函数，这是因为只有当线程的父进程指向"Kthreadd"时，该线程才算是内核线程，而恰好daemonize()函数主要工作便是将该线程的父进程改成"kthreadd"内核线程，默认情况下，调用deamonize()后，会阻塞所有信号，如果想操作某个信号可以调用allow_signal()函数

```
// fn为线程函数，arg为线程函数参数，flags为标记
int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
// name为内核线程的名称
void daemonize(const char * name,...)
```
2. kthread_create接口，则是标准的内核线程创建接口，只须调用该接口便可创建内核线程，默认创建的线程是存于不可运行的状态，所以需要在父进程中通过调用wake_up_process()函数来启动该线程

```
//threadfn为线程函数，data为线程函数参数，namefmt为线程名称，可被格式化的，类似printk一样传入某种格式的线程名
struct task_struct *kthread_create(int (*threadfn)(void *data),void *data, const char namefmt[], ...)

// 唤醒线程
int wake_up_process(struct task_struct *p)
```
3. kthread_run接口创建并启动线程，线程一旦启动起来后，会一直运行，除非该线程主动调用do_exit函数，或者其他的进程调用kthread_stop函数，结束线程的运行

```
// kthread_stop() 通过发送信号给线程
// 如果线程函数正在处理一个非常重要的任务，它不会被中断的，当然如果线程函数永远不返回并且不检查信号，它将永远都不会停止
int kthread_stop(struct task_struct *thread)

// 是以上两个函数的功能的总和
struct task_struct *kthread_run(int(*threadfn)(void*data), void *data, const char namefmt[], ...)
```

> [Linux下的进程类别（内核线程、轻量级进程和用户进程）以及其创建方式](https://blog.csdn.net/gatieme/article/details/51482122)   
> [Linux进程与线程的区别](https://my.oschina.net/cnyinlinux/blog/422207)   
> [Linux 线程的实质](https://my.oschina.net/cnyinlinux/blog/367910)

---
## 进程、线程、协程调度开销

- **调度开销**
1. 进程调度后，CPU执行任务调度的开销，主要是进程上下文切换的开销，以及进程调度使用的是不同的资源，CPU Cache/TLB不命中，导致缺页中断的开销
2. 线程调度后，CPU执行任务调度的开销，主要是进程上下文切换的开销，线程调度的struct task_struct都使用相同的资源，意味着CPU Cache/TLB命中的概率会高很多
3. 协程调度后：    
(1)CPU执行任务调度的开销，主要是寄存器上下文切换的开销，比线程struct task_struct小   
(2)协程分配的栈几kb，线程栈分配几兆，栈切换开销协程小   
(3)协程调度在用户空间，线程调度需要切换用户空间到内核空间，每个进程都会有两个栈，一个内核态栈和一个用户态栈，切换的时候需要切换到内核态栈，同时在进入内核得时候需要保存用户态得寄存器，在内核态返回用户态得时候会恢复这些寄存器得内容，进入内核态内核代码对用户不信任，需要进行额外的检查，系统调用的返回过程有很多额外工作，比如检查是否需要调度等

> [线程调度为什么比进程调度更少开销？](https://www.cnblogs.com/gmpy/p/10265284.html)   
> [协程究竟比线程能省多少开销？](https://blog.csdn.net/zhangyanfei01/article/details/100106207)   
> [为什么用户态和内核态的切换耗费时间？](https://blog.csdn.net/jerry010101/article/details/88946317)

---
## Linux 系统调用
1. 用户代码调用fork()
2. linux则使用int 0x80来触发所有的系统调用
3. 在内核中，有一个数组称为中断向量表，当中断到来时，CPU会暂时中断当前执行的代码，根据中断的中断号，在中断向量表中找到对应的中断处理程序，并调用它，找到中断服务程序
4. 由于中断号是有限的，操作系统不舍得用一个中断号来对应一个系统调用，而倾向于用一个或少数几个中断号对应所有的系统调用，和中断一样，系统调用都有一个系统调用号，每个系统调用号都唯一对应一个系统调用处理函数sys_fork()
5. 返回系统调用

> [深入浅出系统调用的原理](https://blog.csdn.net/kang___xi/article/details/80556633)
---
## Linux 特殊进程

- **idle进程(PID = 0)**
1. idle进程由系统自动创建，运行在内核态，其前身是系统创建的第一个进程，也是唯一一个没有通过fork或者kernel_thread产生的进程，完成加载系统后，演变为进程调度、交换

- **init进程(PID = 1)**
1. init进程由idle进程(PID = 0)通过kernel_thread创建，在内核空间完成初始化后，转向用户空间启动init进程，再启动其他系统进程
2. Linux中的所有进程都是有init进程创建并运行的，是系统中所有其它用户进程的祖先进程，在系统启动完成完成后，init将变为守护进程监视系统其他进程

- **kthreadd(PID = 2)**
1. kthreadd进程由idle进程(PID = 0)通过kernel_thread创建，并始终运行在内核空间，负责所有内核线程的调度和管理
2. 它的任务就是管理和调度其他内核线程kernel_thread，会循环执行一个kthread的函数，该函数的作用就是运行kthread_create_list全局链表中维护的kthread，当我们调用kernel_thread创建的内核线程会被加入到此链表中，因此所有的内核线程都是直接或者间接的以kthreadd为父进程

---
## idle进程(PID = 0)

- **idle进程的创建**
1. idle 进程或因为历史的原因叫做swapper 进程，它是在 linux 的初始化阶段从无到有的创建的一个内核线程，这个祖先进程使用静态分配的数据结构
2. 0号进程上下文信息–init_task描述符：在内核初始化过程中，通过静态定义构造出了一个task_struct接口，取名为init_task，然后在内核初始化的后期，通过rest_init（）函数新建了内核init线程，kthreadd内核线程，完成初始化后，最终演变为0号进程idle，让出CPU，自己进入睡眠，不停的循环，新创建的1号进程kernel_init将会逐个启动次CPU，并最终创建用户进程
3. 在多处理器系统中，每个CPU都有一个进程0，主要打开机器电源，计算机的BIOS就启动一个CPU，同时禁用其他CPU，运行的CPU上的swapper进程初初始化内核数据结构，然后激活其他的并且使用copy_process()函数创建另外的swapper进程，把0传递给新创建的swapper进程作为他们进程的PID

- **0号进程的演化**
1. rest_init创建init进程(PID =1)和kthread进程(PID=2)

```
static noinline void __init_refok rest_init(void)
{
    int pid;

    // 创建1号内核线程，该线程随后转向用户空间，演变为init进程
    kernel_thread(kernel_init, NULL, CLONE_FS);
    numa_default_policy();
    // 创建1号内核线程kthreadd
    pid = kernel_thread(kthreadd, NULL, CLONE_FS | CLONE_FILES);
    rcu_read_lock();
    kthreadd_task = find_task_by_pid_ns(pid, &init_pid_ns);
    rcu_read_unlock();
    complete(&kthreadd_done);

    // 当前0号进程init_task最终会退化成idle进程
    init_idle_bootup_task(current);
    // 切换当前进程，1号进程kernel_init将会运行
    schedule_preempt_disabled();

    // 0号线程进入idle函数的循环，在该循环中会周期性地检查
    cpu_startup_entry(CPUHP_ONLINE);
}
```
2. 创建kernel_init线程完成linux的各项配置(包括启动AP)后，使用sys_execve函数改变核心进程的正文段，将核心进程kernel_init转换成用户进程init，此时处于内核态的1号kernel_init进程将会转换为用户空间内的1号进程init，用户进程init将根据/etc/inittab中提供的信息完成应用程序的初始化调用，然后init进程会执行/bin/sh产生shell界面提供给用户来与Linux系统进行交互
3. 创建kthreadd，它的任务就是管理和调度其他内核线程kernel_thread，会循环执行一个kthread的函数，该函数的作用就是运行kthread_create_list全局链表中维护的kthread，当我们调用kernel_thread创建的内核线程会被加入到此链表中，因此所有的内核线程都是直接或者间接的以kthreadd为父进程
4. 0号进程演变为idle，init_idle_bootup_task，让init_task进程隶属到idle调度类中，调用cpu_idle_loop()，0号线程进入idle函数的循环

- **idle的运行与调度**
1. idle是一个进程，其pid为0
2. 主处理器上的idle由原始进程(pid=0)演变而来，从处理器上的idle由init进程fork得到，但是它们的pid都为0
3. idle进程为最低优先级，且不参与调度，只是在运行队列为空的时候才被调度，linux进程的调度顺序是按照 rt实时进程(rt调度器)， normal普通进程(cfs调度器)，和idel的顺序来调度的，如果rt和cfs都没有可以运行的任务，那么idle才可以被调度

> [Linux下0号进程的前世(init_task进程)今生(idle进程)](https://blog.csdn.net/gatieme/article/details/51484562)


---
## 1号进程的前世(kernel_init)今生(init进程)

- **kernel_init**
1. 由0号进程创建1号进程（内核态），1号内核线程负责执行内核的部分初始化工作、设备驱动程序的初始化以及进行系统配置，并创建若干个用于高速缓存和虚拟主存管理的内核线程

- **init进程**
1. 1号进程调用do_execve运行可执行程序init，并演变成用户态1号进程，即/sbin/init进程，init进程是linux内核启动的第一个用户级进程，init按照配置文件/etc/inittab的要求，完成系统启动工作，比如像启动getty（用于用户登录）、实现运行级别、以及处理孤立进程
2. 每个getty进程设置其进程组标识号，并监视配置到系统终端的接口线路，当检测到来自终端的连接信号时，getty进程将通过函数do_execve()执行注册程序login，此时用户就可输入注册名和密码进入登录过程，如果登陆成功，由login程序再通过函数execv()执行shell，该shell进程接收getty进程的pid，取代原来的getty进程，再由shell直接或间接地产生其他进程
3. 上述过程可描述为：0号进程->1号内核进程->1号用户进程（init进程）->getty进程->shell进程

> [Linux下1号进程的前世(kernel_init)今生(init进程)](https://blog.csdn.net/gatieme/article/details/51532804)

---
## 2号进程kthreadd

- **kthreadd进程事件循环**

```
// kthreadd循环遍历kthread_create_list链表创建线程
int kthreadd(void *unused)
{
    struct task_struct *tsk = current;
    // 允许kthreadd在任意CPU上运行
    set_cpus_allowed_ptr(tsk, cpu_all_mask);
    
    for (;;) {
        // 首先将线程状态设置为 TASK_INTERRUPTIBLE
        set_current_state(TASK_INTERRUPTIBLE);
        // 如果当前没有要创建的线程则主动放弃CPU完成调度，此进程变为阻塞态
        if (list_empty(&kthread_create_list))
            schedule(); 
        
        // 设置进程运行状态为 TASK_RUNNING
        __set_current_state(TASK_RUNNING);
        
        // 加锁
        spin_lock(&kthread_create_lock);
        while (!list_empty(&kthread_create_list)) {
            struct kthread_create_info *create;
            // 从链表中取得 kthread_create_info 结构的地址，并移除
            create = list_entry(kthread_create_list.next, struct kthread_create_info, list);
            list_del_init(&create->list);
            
            // 完成真正线程的创建
            create_kthread(create);
        }
        spin_unlock(&kthread_create_lock);
    }
    return 0;
}

// 完成内核线程创建
static void create_kthread(struct kthread_create_info *create)
{
    int pid;
    // 首先构造一个上下文执行环境
    // 最后调用 do_fork()，返回进程 id
    // 创建后的线程执行 kthread 函数
    pid = kernel_thread(kthread, create, CLONE_FS | CLONE_FILES | SIGCHLD);
}

// 新创建的内核线程执行kthread
static int kthread(void *_create)
{
    // create 指向 kthread_create_info
    struct kthread_create_info *create = _create;
    
    // 新的线程创建完毕后执行的函数
    int (*threadfn)(void *data) = create->threadfn;
    // 新的线程执行的参数
    void *data = create->data;
    
    // 设置运行状态为 TASK_UNINTERRUPTIBLE
    __set_current_state(TASK_UNINTERRUPTIBLE);
    
    // current 表示当前新创建的 thread 的 task_struct 结构
    create->result = current;
    
    // 至此线程创建完毕，执行任务切换，让出 CPU
    schedule();
    
    // 使用wake_up_process(p);唤醒新创建的线程
    // 线程被唤醒后, 会接着执行threadfn(data)
    ret = -EINTR;
    if (!test_bit(KTHREAD_SHOULD_STOP, &self.flags)) {
            __kthread_parkme(&self);
            ret = threadfn(data);
    }
    do_exit(ret);
}
```
1. 我们在内核中通过kernel_create或者其他方式创建一个内核线程，由于新创建的线程还没有创建完毕而继续睡眠，唤醒kthreadd的线程
2. kthreadd内核线程被唤醒，执行内核线程创建的真正工作
3. 新的线程将执行kthread函数，完成创建初始化工作，创建完毕后让出CPU睡眠，因此新的内核线程不会立刻运行
4. 使用wake_up_process(p)唤醒新创建的线程
5. 新线程被唤醒后, 会接着执行threadfn(data)

> [Linux下2号进程的kthreadd](https://blog.csdn.net/gatieme/article/details/51566690)

---
## Linux中fork，vfork和clone

系统调用 | 描述
---|---
fork | fork创造的子进程是父进程的完整副本，复制了父亲进程的资源，包括内存的内容task_struct内容
vfork | vfork创建的子进程与父进程共享数据段，而且由vfork()创建的子进程将先于父进程运行，vfork创造出来的是轻量级进程，也叫线程，是共享资源的进程
clone | Linux上创建线程一般使用的是pthread库，实际上linux也给我们提供了创建线程的系统调用，就是clone

- **fork与vfork区别**
1. fork()子进程拷贝父进程的数据段，代码段，vfork()子进程与父进程共享数据段（所有内存包括栈地址），直至子进程使用execve启动新的应用程序为止
2. fork()父子进程的执行次序不确定，vfork()保证子进程先运行，父进程被挂起，除非子进程调用exit或者execve才会唤起父进程
3. 如果在调用vfork时子进程依赖于父进程的进一步动作，则会导致死锁，因此这个系统调用是用来启动一个新的应用程序，其次子进程在vfork()返回后直接运行在父进程的栈空间，并使用父进程的内存和数据，这意味着子进程可能破坏父进程的数据结构或栈，造成失败，为了避免这些问题，需要确保一旦调用vfork()，子进程就不从当前的栈框架中返回，并且如果子进程改变了父进程的数据结构就不能调用exit函数

- **为什么会有vfork**
1. 因为以前的fork当它创建一个子进程时，将会创建一个新的地址空间，并且拷贝父进程的资源，而往往在子进程中会执行exec调用，这样，前面的拷贝工作就是白费力气了，这种情况下，聪明的人就想出了vfork，它产生的子进程刚开始暂时与父进程共享地址空间（其实就是线程的概念了）
2. 用vfork函数创建子进程后，子进程往往要调用一种exec函数以执行另一个程序，exec函数用另一个新程序替换了当前进程的正文，数据，堆和栈段，因为调用exec并不创建新进程，所以前后的进程id 并未改变
3. vfork设计用以子进程创建后立即执行execve系统调用加载新程序的情形，在子进程退出或开始新程序之前，内核保证了父进程处于阻塞状态

- **clone**
1. clone可以选择性的继承父进程的资源，可以选择想vfork一样和父进程共享一个虚存空间，从而使创造的是线程，也可以不和父进程共享，甚至可以选择创造出来的进程和父进程不再是父子关系，而是兄弟关系
```
// fn是函数指针
// child_stack是为子进程分配系统堆栈空间（在linux下系统堆栈空间是2页面，就是8K的内存，其中在这块内存中，低地址上放入了进程控制块task_struct的值）
// flags就是标志用来描述你需要从父进程继承那些资源
// arg就是传给子进程的参数
int clone(int (fn)(void ), void *child_stack, int flags, void *arg)
```

- **fork，vfork和clone 具体实现参数区别**
1. sys_fork中clone_flags为SIGCHLD，用户栈为父进程栈（copy on write），parent_tidptr，child_ctidptr都是NULL
2. vfork中clone_flags为SIGCHLD | CLONE_VFORK | CLONE_VM（共享内存描述符和所有页表），用户栈为父进程栈，parent_tidptr，child_ctidptr都是NULL
3. sysclone中clone_flags参数由用户自定义，可以指定新的栈，并把parent_tidptr，child_tidptr都传到了 do_fork的参数中

> [Linux中fork，vfork和clone详解（区别与联系）](https://blog.csdn.net/gatieme/article/details/51417488)



---
## 进程的创建过程分析(_do_fork/do_fork详解)

- **系统调用的参数传递**
1. fork、vfork和clone的系统调用的入口地址分别是sys_fork、sys_vfork和sys_clone，而他们的定义是依赖于体系结构的，因为在用户空间和内核空间之间传递参数的方法因体系结构而异
2. 普通C函数通过将参数的值压入到进程的栈中进行参数的传递，而系统调用是通过中断进程从用户态到内核态的一种特殊的函数调用，在进行系统调用之前，系统调用的参数被写入CPU的寄存器，而在实际调用系统服务例程之前，内核将CPU寄存器的内容拷贝到内核堆栈中，实现参数的传递

- **sys_fork的实现**

```
// 唯一使用的标志是SIGCHLD，意味着在子进程终止后将发送信号SIGCHLD信号通知父进程
// 如果do_fork成功，则新建进程的pid作为系统调用的结果返回，否则返回错误码

#ifdef __ARCH_WANT_SYS_FORK
SYSCALL_DEFINE0(fork)
{
#ifdef CONFIG_MMU
        return _do_fork(SIGCHLD, 0, 0, NULL, NULL, 0);
#else
        /* can not support in nommu mode */
        return -EINVAL;
#endif
}
#endif
```
- **sys_vfork的实现**

```
// 使用了额外的标志CLONE_VFORK | CLONE_VM

#ifdef __ARCH_WANT_SYS_VFORK
SYSCALL_DEFINE0(vfork)
{
        return _do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, 0, 0, NULL, NULL, 0);
}
#endif
```
- **sys_clone的实现**

```
// sys_clone的标识通过各个寄存器参数传递到系统调用，因而我们需要提取这些参数
// clone也不再复制进程的栈，而是可以指定新的栈地址
// 另外还指令了用户空间的两个指针(parent_tidptr和child_tidptr)，用于与线程库通信

#ifdef __ARCH_WANT_SYS_CLONE
#ifdef CONFIG_CLONE_BACKWARDS
SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
                 int __user *, parent_tidptr,
                 unsigned long, tls,
                 int __user *, child_tidptr)
#elif defined(CONFIG_CLONE_BACKWARDS2)
SYSCALL_DEFINE5(clone, unsigned long, newsp, unsigned long, clone_flags,
                 int __user *, parent_tidptr,
                 int __user *, child_tidptr,
                 unsigned long, tls)
#elif defined(CONFIG_CLONE_BACKWARDS3)
SYSCALL_DEFINE6(clone, unsigned long, clone_flags, unsigned long, newsp,
                int, stack_size,
                int __user *, parent_tidptr,
                int __user *, child_tidptr,
                unsigned long, tls)
#else
SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
                 int __user *, parent_tidptr,
                 int __user *, child_tidptr,
                 unsigned long, tls)
#endif
{
        return _do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr, tls);
}
#endif
```

- **创建子进程的流程_do_fork**

```
// 创建子进程
long _do_fork(unsigned long clone_flags,
      unsigned long stack_start,
      unsigned long stack_size,
      int __user *parent_tidptr,
      int __user *child_tidptr,
      unsigned long tls)
{
    struct task_struct *p;
    int trace = 0;
    long nr;
    
    // 根据clone_flags设置trace
    if (!(clone_flags & CLONE_UNTRACED)) {
    if (clone_flags & CLONE_VFORK)
        trace = PTRACE_EVENT_VFORK;
    else if ((clone_flags & CSIGNAL) != SIGCHLD)
        trace = PTRACE_EVENT_CLONE;
    else
        trace = PTRACE_EVENT_FORK;

    if (likely(!ptrace_event_enabled(current, trace)))
        trace = 0;
    }
    
    // 复制进程描述符，返回值是一个 task_struct 指针
    p = copy_process(clone_flags, stack_start, stack_size,
         child_tidptr, NULL, trace, tls);
         
    // 如果是 vfork（设置了CLONE_VFORK和ptrace标志）初始化完成处理信息
    if (clone_flags & CLONE_VFORK) {
        p->vfork_done = &vfork;
        init_completion(&vfork);
        get_task_struct(p);
    }
    
    // 将子进程加入调度器，为之分配 CPU
    wake_up_new_task(p);
    
    // 如果是 vfork，将父进程加入至等待队列，等待子进程完成
    if (clone_flags & CLONE_VFORK) {
        if (!wait_for_vfork_done(p, &vfork))
        ptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);
    }
}

// 复制进程
static struct task_struct *copy_process(unsigned long clone_flags,
                    unsigned long stack_start,
                    unsigned long stack_size,
                    int __user *child_tidptr,
                    struct pid *pid,
                    int trace,
                    unsigned long tls)
{
    int retval;
    struct task_struct *p;
    
    // 复制当前的 task_struct 和 thread_info
    // 子进程除了task_struct->stack栈指针与父进程不同之外，全部都一样
    retval = -ENOMEM;
    p = dup_task_struct(current);
    
    // 检查进程数是否超过限制，由操作系统定义
    retval = -EAGAIN;
    if (atomic_read(&p->real_cred->user->processes) >=
            task_rlimit(p, RLIMIT_NPROC)) {
        if (p->real_cred->user != INIT_USER &&
            !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))
            goto bad_fork_free;
    }
    current->flags &= ~PF_NPROC_EXCEEDED;
    
    //  初始化自旋锁
    spin_lock_init(&p->alloc_lock);
    //  初始化挂起信号
    init_sigpending(&p->pending);
    //  初始化 CPU 定时器
    posix_cpu_timers_init(p);
    
    // 设置子进程调度相关的参数（子进程的运行CPU、初始时间片长度和静态优先级等），并把进程状态设置为 TASK_RUNNING
    retval = sched_fork(clone_flags, p);
    
    // 复制所有进程信息，包括文件系统、信号处理函数、信号、内存管（包括页表、地址空间和代码数据）、namespace、io等
    retval = audit_alloc(p);
    retval = copy_semundo(clone_flags, p);
    retval = copy_files(clone_flags, p);
    retval = copy_fs(clone_flags, p);
    retval = copy_sighand(clone_flags, p);
    retval = copy_signal(clone_flags, p);
    retval = copy_mm(clone_flags, p);
    retval = copy_namespaces(clone_flags, p);
    retval = copy_io(clone_flags, p);
    
    // 初始化子进程内核栈
    retval = copy_thread_tls(clone_flags, stack_start, stack_size, p, tls);
    
    // 为新进程分配并设置新的 pid
    pid = alloc_pid(p->nsproxy->pid_ns_for_children);
    
    // 设置task_struct的pid、tgid、group_leader
    p->pid = pid_nr(pid);
    if (clone_flags & CLONE_THREAD) {
        p->exit_signal = -1;
        p->group_leader = current->group_leader;
        p->tgid = current->tgid;
    } else {
        if (clone_flags & CLONE_PARENT)
            p->exit_signal = current->group_leader->exit_signal;
        else
            p->exit_signal = (clone_flags & CSIGNAL);
        p->group_leader = p;
        p->tgid = p->pid;
    }
    
    // 设置task_struct的real_parent
    if (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {
        p->real_parent = current->real_parent;
        p->parent_exec_id = current->parent_exec_id;
    } else {
        p->real_parent = current;
        p->parent_exec_id = current->self_exec_id;
    }
}

union thread_union {
   struct thread_info thread_info;
  unsigned long stack[THREAD_SIZE/sizeof(long)];
};

// 复制当前的 task_struct 和 thread_info，并分配新的 stack
static struct task_struct *dup_task_struct(struct task_struct *orig)
{
    struct task_struct *tsk;
    struct thread_info *ti;
    int node = tsk_fork_get_node(orig);
    int err;

    //分配一个task_struct节点
    tsk = alloc_task_struct_node(node);
    if (!tsk)
        return NULL;

    //分配一个 thread_info节点，其实是分配了一个thread_union联合体，包含进程的内核栈，ti 为栈底
    ti = alloc_thread_info_node(tsk, node);
    if (!ti)
        goto free_tsk;
    
    //将栈底的值赋给新节点的栈
    tsk->stack = ti;

    //……

    return tsk;

}

// 设置子进程调度相关的参数（子进程的运行CPU、初始时间片长度和静态优先级等），并把进程状态设置为 TASK_RUNNING，为子进程分配cpu
int sched_fork(unsigned long clone_flags, struct task_struct *p)
{
    unsigned long flags;
    int cpu = get_cpu();

    __sched_fork(clone_flags, p);

    //  将子进程状态设置为 TASK_RUNNING
    p->state = TASK_RUNNING;

    //  ……

    //  为子进程分配 CPU
    set_task_cpu(p, cpu);

    put_cpu();
    return 0;
}

// 这是一个特定于体系结构的函数，用于复制进程中特定于线程(thread-special)的数据，重要的就是填充task_struct->thread的各个成员
// thread_struct定义是依赖于体系结构的，它包含了所有寄存器(和其他信息)，内核在进程之间切换时需要保存和恢复的进程的信息
// 设置子进程的执行环境，如子进程运行时各CPU寄存器的值、子进程的内核栈的起始地址（指向内核栈的指针通常也是保存在一个特别保留的寄存器中）
int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
    unsigned long arg, struct task_struct *p, unsigned long tls)
{
    struct pt_regs *childregs = task_pt_regs(p);
    struct task_struct *tsk;
    int err;
    //  获取寄存器的信息
    p->thread.sp = (unsigned long) childregs;
    p->thread.sp0 = (unsigned long) (childregs+1);
    memset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));

    // 内核线程的设置
    if (unlikely(p->flags & PF_KTHREAD)) {
        memset(childregs, 0, sizeof(struct pt_regs));
        p->thread.ip = (unsigned long) ret_from_kernel_thread;
        task_user_gs(p) = __KERNEL_STACK_CANARY;
        childregs->ds = __USER_DS;
        childregs->es = __USER_DS;
        childregs->fs = __KERNEL_PERCPU;
        childregs->bx = sp;     /* function */
        childregs->bp = arg;
        childregs->orig_ax = -1;
        childregs->cs = __KERNEL_CS | get_kernel_rpl();
        childregs->flags = X86_EFLAGS_IF | X86_EFLAGS_FIXED;
        p->thread.io_bitmap_ptr = NULL;
        return 0;
    }
    // 将当前寄存器信息复制给子进程
    *childregs = *current_pt_regs();
    // 子进程 eax 置 0，因此fork 在子进程返回0
    childregs->ax = 0;
    if (sp)
        childregs->sp = sp;
    // 子进程ip 设置为ret_from_fork，因此子进程从ret_from_fork开始执行
    p->thread.ip = (unsigned long) ret_from_fork;
    task_user_gs(p) = get_user_gs(current_pt_regs());
    err = 0;

    // 为进程设置一个新的TLS
    if (clone_flags & CLONE_SETTLS)
        err = do_set_thread_area(p, -1,
            (struct user_desc __user *)tls, 0);

    if (err && p->thread.io_bitmap_ptr) {
        kfree(p->thread.io_bitmap_ptr);
        p->thread.io_bitmap_max = 0;
    }
    return err;
}
```

> [Linux下进程的创建过程分析(_do_fork/do_fork详解)](https://blog.csdn.net/gatieme/article/details/51577479)


---
## Linux进程内核栈与thread_info结构

- **为什么需要内核栈**
1. 用户态进程所用的栈，是在进程线性地址空间中，内核态的进程访问处于内核数据段的栈
2. 内核栈是当进程从用户空间进入内核空间时，特权级发生变化，需要切换堆栈，那么内核空间中使用的就是这个内核栈
3. 内核态堆栈仅用于内核例程，Linux内核另外为中断提供了单独的硬中断栈和软中断栈

- **为什么需要thread_info**
1. 内核还需要存储每个进程的PCB(进程控制块 process control block)信息，linux内核是支持不同体系的的，但是不同的体系结构可能进程需要存储的信息不尽相同，这就需要我们实现一种通用的方式，我们将体系结构相关的部分和无关的部门进行分离
2. thread_info就保存了特定体系结构的汇编代码段需要访问的那部分进程的数据（通用寄存器，指令计数器，PSW，用户的栈指针）

- **thread_union**
1. linux将内核栈和进程控制块thread_info融合在一起，组成一个联合体thread_union

```
union thread_union
{
    // 线程描述符 thread_info
    struct thread_info thread_info;
    // 内核态的进程堆栈stack
    unsigned long stack[THREAD_SIZE/sizeof(long)];
};
```
2. thread_union 这块区域32位上通常是8K=8192（占两个页框），64位上通常是16K，其实地址必须是8192的整数倍，出于效率考虑，内核让这8K(或者16K)空间占据连续的两个页框并让第一个页框的起始地址是213的倍数

```
    +-------------------+ 0x015fbfff
    |                   |
    |                   |
    |                   |
    |       堆栈⬇       | 
    |                   |
    |                   |
    |-------------------| 0x015fa878    ⬅ esp
    |                   |
    |                   |
    |                   |
    |                   |
    |                   |
    |-------------------| 0x015fa034
    |    thread_info    |
    |                   |
    +-------------------+ 0x015fa000    ⬅ current   task_struct.stack
    

// esp寄存器是CPU栈指针，用来存放栈顶单元的地址，从用户态刚切换到内核态以后，进程的内核栈总是空的，esp寄存器指向这个栈的顶端，一旦数据写入堆栈，esp的值就递减
// thread_info和内核栈虽然共用了thread_union结构，但是thread_info大小固定，存储在联合体的开始部分，而内核栈由高地址向低地址扩展，当内核栈的栈顶到达thread_info的存储空间时，则会发生栈溢出
// 系统的current指针指向了当前运行进程的thread_union(或者thread_info)的地址
// 进程task_struct中的stack指针指向了进程的thread_union(或者thread_info)的地址
```

- **内核栈与thread_info的通用操作**

```
// 未定义__HAVE_THREAD_FUNCTIONS的时候使用内核的默认操作
#ifndef __HAVE_THREAD_FUNCTIONS  

// 通过进程的task_struct来获取进程的thread_info
#define task_thread_info(task)  ((struct thread_info *)(task)->stack)
// 通过进程的task_struct来获取进程的内核栈
#define task_stack_page(task)   ((task)->stack)

// 初始化thread_info, 指定其存储结构的内存布局
static inline void setup_thread_stack(struct task_struct *p, struct task_struct *org)
{
    *task_thread_info(p) = *task_thread_info(org);
    task_thread_info(p)->task = p;
}

// 获取栈底
static inline unsigned long *end_of_stack(struct task_struct *p)
{
#ifdef CONFIG_STACK_GROWSUP
    return (unsigned long *)((unsigned long)task_thread_info(p) + THREAD_SIZE) - 1;
#else
    return (unsigned long *)(task_thread_info(p) + 1);
#endif
}

#endif

// 在内核的某个特定组建使用了较多的栈空间时，内核栈会溢出到thread_info部分
// 判断给出的地址是否位于栈的有效部分
#ifndef __HAVE_ARCH_KSTACK_END
static inline int kstack_end(void *addr)
{
    /* Reliable end of stack detection:
     * Some APM bios versions misalign the stack
     */
    return !(((unsigned long)addr+sizeof(void*)-1) & (THREAD_SIZE-sizeof(void*)));
}
#endif

// 获取当前在CPU上正在运行进程的thread_info
current_thread_info(void)
    return (struct thread_info *)(current_top_of_stack() - THREAD_SIZE);
```

- **内核栈分配与销毁**

```
# if THREAD_SIZE >= PAGE_SIZE
static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
                          int node)
{
    struct page *page = alloc_kmem_pages_node(node, THREADINFO_GFP,
                          THREAD_SIZE_ORDER);

    return page ? page_address(page) : NULL;
}

static inline void free_thread_info(struct thread_info *ti)
{
    free_kmem_pages((unsigned long)ti, THREAD_SIZE_ORDER);
}
# else
static struct kmem_cache *thread_info_cache;

static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
                          int node)
{
    return kmem_cache_alloc_node(thread_info_cache, THREADINFO_GFP, node);
}

static void free_thread_info(struct thread_info *ti)
{
    kmem_cache_free(thread_info_cache, ti);
}
```

> [Linux进程内核栈与thread_info结构详解](https://blog.csdn.net/gatieme/article/details/51577479)


---
## Linux内核线程kernel thread

- **kernel thread**
1. 内核线程只运行在内核态，并且只能使用大于PAGE_OFFSET（传统的x86_32上是3G）的地址空间
2. 内核线程执行的任务：   
(1)周期性地将修改的内存页与页来源块设备同步   
(2)如果内存页很少使用，则写入交换区   
(3)管理延时动作，如２号进程接手内核进程的创建   
(4)实现文件系统的事务日志   

- **内核线程的进程描述符task_struct**

```
struct task_struct
{
    // 对于普通用户进程来说，mm指向虚拟地址空间的用户空间部分
    // 由于内核线程之前可能是任何用户层进程在执行，故用户空间部分的内容本质上是随机的，内核线程决不能修改其内容，因此内核线程mm为NULL
    struct mm_struct *mm;
    // active_mm主要用于优化，内核将原来进程的mm存放在新内核线程的active_mm中
    // 假如内核线程之后运行的进程与之前是同一个，在这种情况下，内核并不需要修改用户空间地址表，地址转换后备缓冲器(TLB)中的信息仍然有效
    // 只有在内核线程之后，执行的进程是与此前不同的用户层进程时，才需要切换(并对应清除TLB数据)
    struct mm_struct *avtive_mm;
};
```

- **kernel_thread**

```
// 上文2号线程 kthreadd - kthread - kernel_thread
// kernel_thread是最基础的创建内核线程的接口
// 通过_do_fork创建一个线程，创建的线程运行在内核空间，并且与其他进程线程共享内核虚拟地址空间

pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
{
    return _do_fork(flags|CLONE_VM|CLONE_UNTRACED, (unsigned long)fn, (unsigned long)arg, NULL, NULL, 0);
}
```

- **线程创建**

```
// 创建线程并立即唤醒它
#define kthread_run(threadfn, data, namefmt, ...)                          \
({                                                                         \
    struct task_struct *__k                                            \
            = kthread_create(threadfn, data, namefmt, ## __VA_ARGS__); \
    if (!IS_ERR(__k))                                                  \
            wake_up_process(__k);                                      \
    __k;                                                               \
})


```

> [Linux内核线程kernel thread详解](https://blog.csdn.net/gatieme/article/details/51589205)


---
## 进程退出(do_exit)

- **linux下进程退出的方式**
1. 正常退出：return、exit、_exit
2. 异常退出：abort、信号终止

> [Linux进程退出详解(do_exit)](https://blog.csdn.net/gatieme/article/details/51638706)

---
## Linux僵尸进程与孤儿进程

- **基本概念**
1. 僵尸进程：一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中，这种进程称之为僵死进程
2. 孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程，孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作

- **僵尸进程危害与场景**
1. 危害：任何一个子进程(init除外)在exit()之后，如果父进程不调用wait / waitpid的话，那么保留的那段信息就不会释放，其进程号就会一直被占用，但是系统所能使用的进程号是有限的，如果大量的产生僵死进程，将因为没有可用的进程号而导致系统不能产生新的进程
2. 场景：有个进程，它定期的产生一个子进程，这个子进程需要做的事情很少，做完它该做的事情之后就退出了，因此这个子进程的生命周期很短，但是，父进程只管生成新的子进程，至于子进程 退出之后的事情，则一概不闻不问，这样，系统运行上一段时间之后，系统中就会存在很多的僵死进程

- **处理或预防僵尸进程**
1. ps -A -ostat,ppid,pid,cmd | grep -e '^ [Zz]'（-A 参数列出所有进程，状态为z或者Z的进程为僵尸进程），kill僵尸进程
2. 在父进程创建子进程之前，就向系统申明自己并不会对这个子进程的exit动作进行任何关注行为，这样的话，子进程一旦退出后，系统就不会去等待父进程的操作，而是直接将该子进程的资源回收掉，也就不会出现僵尸进程了：父进程调用 signal(SIGCHLD,SIG_IGN)
3. 创建完子进程后，用waitpid等待子进程返回

> [linux上寻找并杀死僵尸进程](https://blog.csdn.net/shanzhizi/article/details/47320595)   
> [linux下的僵尸进程产生原因和解决方法](https://blog.csdn.net/LEON1741/article/details/78142269)   
> [孤儿进程与僵尸进程产生及其处理](https://blog.csdn.net/Eunice_fan1207/article/details/81387417)   
> [wait和waitpid函数的用法和总结](https://blog.csdn.net/u011068702/article/details/54409273)

---
## Linux进程调度器

- **2个调度器**
1. 一种是直接的，比如进程打算睡眠或出于其他原因放弃CPU
2. 另一种是通过周期性的机制，以固定的频率运行，不时的检测是否有必要
3. 当前linux的调度程序由两个调度器组成：主调度器，周期性调度器(两者又统称为通用调度器(generic scheduler)或核心调度器(core scheduler))，并且每个调度器包括两个内容：调度框架(其实质就是两个函数框架)及调度器类

- **6种调度策略**

字段 | 描述 | 所在调度器类
---|---|---
SCHED_NORMAL（调度普通的非实时进程） | 用于普通进程，通过CFS调度器实现，SCHED_BATCH用于非交互的处理器消耗型进程，SCHED_IDLE是在系统负载很低时使用 | CFS
SCHED_BATCH（调度普通的非实时进程） | SCHED_NORMAL普通进程策略的分化版本，采用分时策略，根据动态优先级(可用nice()API设置），分配CPU运算资源，在有实时进程存在时，实时进程优先调度，但针对吞吐量优化，除了不能抢占外与常规任务一样，允许任务运行更长时间，更好地使用高速缓存，适合于成批处理的工作 | CFS
SCHED_IDLE（在系统空闲时调用idle进程） | 优先级最低，在系统空闲时才跑这类进程 | CFS-IDLE
SCHED_FIFO（采用不同的调度策略调度实时进程） | 先入先出调度算法（实时调度策略），相同优先级的任务先到先服务，高优先级的任务可以抢占低优先级的任务 | RT
SCHED_RR（采用不同的调度策略调度实时进程） | 轮流调度算法（实时调度策略），不同要求的实时任务可以根据需要用sched_setscheduler() API设置策略 | RT
SCHED_DEADLINE（采用不同的调度策略调度实时进程） | 新支持的实时进程调度策略，针对突发型计算，且对延迟和完成时间高度敏感的任务适用，基于Earliest Deadline First (EDF最早截止期限优先调度算法) 调度算法 | DF

- **5个调度器类**
1. 依据其调度策略的不同实现了5个调度器类，一个调度器类可以用一种种或者多种调度策略调度某一类进程，也可以用于特殊情况或者调度特殊功能的进程

调度器类 | 描述 | 对应调度策略
---|---|---
stop_sched_class | 优先级最高的线程，会中断所有其他线程，且不会被其他任务打断，作用：发生在cpu_stop_cpu_callback 进行cpu之间任务migration和HOTPLUG_CPU的情况下关闭任务 | 无，不需要调度普通进程
dl_sched_class | 采用EDF最早截至时间优先算法调度实时进程 | SCHED_DEADLINE
rt_sched_class | 采用提供Roound-Robin算法或者FIFO算法调度实时进程具体调度策略由进程的task_struct->policy指定 | SCHED_FIFO、SCHED_RR
fair_sched_clas | 采用CFS算法调度普通的非实时进程 | SCHED_NORMAL、SCHED_BATCH
idle_sched_class | 采用CFS算法调度idle进程，每个cup的第一个pid=0线程：swapper，是一个静态线程 | SCHED_IDLE

```
调度优先级
stop_sched_class -> dl_sched_class -> rt_sched_class -> fair_sched_class -> idle_sched_class
```

- **3个调度实体**
1. 调度器不限于调度进程，还可以调度更大的实体，比如实现组调度，这种一般性要求调度器不直接操作进程, 而是处理可调度实体, 因此需要一个通用的数据结构描述这个调度实体,即seched_entity结构, 其实际上就代表了一个调度对象，可以为一个进程，也可以为一个进程组

调度实体 | 名称 | 描述 | 对应调度器类
---|---|---|---
sched_dl_entity | DEADLINE调度实体 | 采用EDF算法调度的实时调度实体 | dl_sched_class
sched_rt_entity | RT调度实体 | 采用Roound-Robin或者FIFO算法调度的实时调度实体 | rt_sched_class
sched_entity | CFS调度实体 | 采用CFS算法调度的普通非实时进程的调度实体 | fair_sched_class

- **调度器类的就绪队列**
1. 对于调度框架及调度器类，它们都有自己管理的运行队列，调度框架只识别rq，而对于cfs调度器类它的运行队列则是cfs_rq（内部使用红黑树组织调度实体），实时rt的运行队列则为rt_rq（内部使用优先级bitmap+双向链表组织调度实体）,此外内核对新增的dl实时调度策略也提供了运行队列dl_rq

- **系统发生调度的时机度**
1. 调用cond_resched()时
2. 显式调用schedule()时
3. 从系统调用或者异常中断返回用户空间时
4. 从中断上下文返回用户空间时
5. 在系统调用或者异常中断上下文中调用preempt_enable()时
6. 在中断上下文中，从中断下半部处理函数返回到可抢占的上下文时


---
## Linux进程调度策略的发展和演变

- **Linux2.4的调度器O(n)**
1. 概述：调度器采用基于优先级的设计，该调度器的pick next算法非常简单：对runqueue中所有进程的优先级进行依次进行比较，选择最高优先级的进程作为下一个被调度的进程
2. 详情：   
(1)每个进程被创建时都被赋予一个时间片，时钟中断递减当前运行进程的时间片，当进程的时间片被用完时，它必须等待重新赋予时间片才能有机会运行，Linux2.4调度器保证只有当所有RUNNING进程的时间片都被用完之后，才对所有进程重新分配时间片，这段时间被称为一个epoch，这种设计保证了每个进程都有机会得到执行，每个epoch中，每个进程允许执行到其时间切片用完，如果某个进程没有使用其所有的时间切片，那么剩余时间切片的一半将被添加到新时间切片使其在下个epoch中可以执行更长时间   
(2)实时进程调度：优先级是静态设定的，而且始终大于普通进程的优先级，采用SCHED_FIFO 和 SCHED_RR两种调度策略   
(3)普通进程调度：普通进程的优先级主要由进程描述符中的时间片Counter字段决定 (还要加上 nice 设定的静态优先级)，进程被创建时子进程的 counter值为父进程counter值的一半，这样保证了任何进程不能依靠不断地 fork() 子进程从而获得更多的执行机会   
(4)提高交互式进程的优先级：当所有 RUNNING 进程的时间片被用完之后，调度器将重新计算所有进程的 counter 值，处于睡眠状态的进程的 counter 本来就没有用完，在重新计算时，他们的 counter 值会加上这些原来未用完的部分，从而提高了它们的优先级，交互式进程经常因等待用户输入而处于睡眠状态，当它们重新被唤醒并进入 runqueue 时，就会优先于其它进程而获得 CPU
3. 缺点：   
(1)调度器选择进程时需要遍历整个 runqueue 从中选出最佳人选，当进程数很大时，系统整体的性能下降   
(2)交互式进程的优化并不完善：比如一个数据库引擎在处理查询时会经常地进行磁盘IO，虽然它们并不需要快速地用户响应，还是被提高了优先级   
(3)对实时进程的支持不够：Linux2.4内核是非抢占的

- **O(1)的调度算法**
1. 概述：普通进程使用100到139的优先级，实时进程使用0到99的优先级，该调度算法为每个优先级都设置一个可运行队列，即包含140个可运行状态的进程链表，每一条优先级链表上的进程都具有相同的优先级，而不同进程链表上的进程都拥有不同的优先级，除此之外, 还包括一个优先级位图bitmap，该位图使用一个位(bit)来代表一个优先级，而140个优先级最少需要5个32位来表示，因此只需要一个int[5]就可以表示位图，该位图中的所有位都被置0，当某个优先级的进程处于可运行状态时，该优先级所对应的位就被置1
2. 详情：   
(1)普通进程优先级计算：   
&emsp;a）每个进程与生俱来（即从父进程那里继承而来）都有一个优先级，即静态优先级，静态优先级本质上决定了时间片分配的大小   
&emsp;b）当调度程序选择新进程运行时就会使用进程的动态优先级，动态优先级的生成是以静态优先级为基础，再加上相应的惩罚或奖励(bonus)，这个bonus根据进程过去的平均睡眠时间做相应的惩罚或奖励，所谓平均睡眠时间（sleep_avg，位于task_struct结构中）就是进程在睡眠状态所消耗的总时间数，平均睡眠时间随着进程的睡眠而增长，随着进程的运行而减少，因此交互性强的进程会得到调度程序的奖励（bonus为正），而那些一直霸占CPU的进程会得到相应的惩罚（bonus为负）   
(2)实时进程的优先级计算：   
&emsp;a）内核2.6中时间片用任务描述符中的time_slice域表示，而优先级用prio（普通进程）或者rt_priority（实时进程）表示，调度器为每一个CPU维护了两个进程队列数组，指向活动运行队列的active数组和指向过期运行队列的expire数组，系统一共有140个不同的优先级，因此这两个数组大小都是140，它们是按照先进先出的顺序进行服务的，被调度执行的任务都会被添加到各自运行队列优先级列表的末尾    
&emsp;b）当需要选择当前最高优先级的进程时，2.6调度器不用遍历整个runqueue，而是直接从active数组中选择当前最高优先级队列中的第一个进程，为了实现O(1)算法active数组维护了一个由5个32位的字（140个优先级）组成的bitmap，当某个优先级别上有进程被插入列表时，相应的比特位就被置位   
(3)提高交互式进程的优先级：   
&emsp;a）O(1)调度器不仅动态地提高该类进程的优先级   
&emsp;b）每次时钟tick中断时，进程的时间片(time_slice)被减一，当time_slice为0时，表示当前进程的时间片用完，调度器判断当前进程的类型，如果是交互式进程或者实时进程，则重置其时间片并重新插入active数组，如果不是交互式进程则从active数组中移到expired数组，并根据上述公式重新计算时间片，这样实时进程和交互式进程就总能优先获得CPU，然而这些进程不能始终留在active数组中，否则进入expire数组的进程就会产生饥饿现象，当进程已经占用CPU时间超过一个固定值后，即使它是实时进程或者交互式进程也会被移到expire数组中，当active数组中的所有进程都被移到expire数组中后，调度器交换active数组和expire数组，因此新的active数组又恢复了初始情况，而expire数组为空，从而开始新的一轮调度
3. 缺点：   
(1)有一些著名的程序总能让该调度器性能下降，导致交互式进程反应缓慢，例如fiftyp.c、thud.c、chew.c   
(2)O(1)调度器对NUMA支持也不完善   
(3)调度器根据平均睡眠时间和一些很难理解的经验公式来修正进程的动态优先级以及区分交互式进程，这样的代码很难阅读和维护

- **Linux 2.6的新一代调度器CFS（Completely Fair Scheduler）之楼梯调度算法**
1. 普通进程楼梯调度算法(staircase scheduler)：当进程用完了自己的时间片后，并不是被移到expire数组中，而是被加入active数组的低一优先级列表中，即将其降低一个级别（注意这里只是将该任务插入低一级优先级任务列表中，任务本身的优先级并没有改变），当时间片再次用完，任务被再次放入更低一级优先级任务队列中，就象一部楼梯，任务每次用完了自己的时间片之后就下一级楼梯，任务下到最低一级楼梯时，如果时间片再次用完，它会回到初始优先级的下一级任务队列中
2. 实时进程的优先级计算：仍然采用SCHED_FIFO 和 SCHED_RR两种调度策略
3. 交互式进程的优先级：对于交互式应用，当进入睡眠状态时，与它同等优先级的其他进程将一步一步地走下楼梯，进入低优先级进程队列，当该交互式进程再次唤醒后，它还留在高处的楼梯台阶上，从而能更快地被调度器选中，加速了响应时间
4. 优点：   
(1)楼梯算法能避免进程饥饿现象，高优先级的进程会最终和低优先级的进程竞争，使得低优先级进程最终获得执行机会   
(2)删除了O(1)调度器中动态修改优先级的复杂代码,还淘汰了expire数组，从而简化了代码
(3)最重要的意义在于证明了完全公平这个思想的可行性

- **Linux 2.6的新一代调度器CFS（Completely Fair Scheduler）之RSDL(Rotating Staircase Deadline Scheduler)**
1. 它为每一个优先级都分配了一个"组时间配额"，记为Tg，同一优先级的每个进程都拥有同样的"优先级时间配额"，用Tp表示，当进程用完了自身的Tp时，就下降到下一优先级进程组中，在RSDL中这个过程叫做minor rotation（次轮询），当高优先级进程组用完了它们的Tg(即组时间配额)时，无论该组中是否还有进程Tp尚未用完，所有属于该组的进程都被强制降低到下一优先级进程组中，这样低优先级任务就可以在一个可以预计的未来得到调度，从而改善了调度的公平性，这就是RSDL中Deadline代表的含义
2. 当进程次轮询到最后一次到最低优先级140时，将进程从active数组中移到expired数组，当active数组为空时，触发主轮询major rotation，交换active数组和expire数组，所有进程都恢复到初始状态，再一次从新开始minor rotation（次轮询）的过程

> [Linux进程调度策略的发展和演变](https://blog.csdn.net/gatieme/article/details/51701149)


---
## Linux进程调度器的设计

- **task_struct中调度相关的成员**

```
struct task_struct
{
    ........
    // 表示是否在运行队列
    int on_rq;

    // 进程优先级 
    // prio: 动态优先级，范围为100~139，与静态优先级和补偿(bonus)有关
    // static_prio: 静态优先级，static_prio = 100 + nice + 20 (nice值为-20~19,所以static_prio值为100~139)
    // normal_prio: 没有受优先级继承影响的常规优先级，具体见normal_prio函数，跟属于什么类型的进程有关
    int prio, static_prio, normal_prio;
    // 实时进程优先级
    unsigned int rt_priority;

    // 调度类，调度处理函数类
    const struct sched_class *sched_class;

    // 调度实体(红黑树的一个结点)
    struct sched_entity se;
    // 调度实体(实时调度使用)
    struct sched_rt_entity rt;
    struct sched_dl_entity dl;

#ifdef CONFIG_CGROUP_SCHED
    // 指向其所在进程组
    struct task_group *sched_task_group;
#endif
    ........
}
```

字段 | 描述
---|---
static_prio | 用于保存静态优先级, 是进程启动时分配的优先级，可以通过nice和sched_setscheduler系统调用来进行修改, 否则在进程运行期间会一直保持恒定
prio | 保存进程的动态优先级（由于在某些情况下内核需要暂时提高进程的优先级, 因此需要用prio表示，这些改变不是持久的）
normal_prio | 表示基于进程的静态优先级static_prio和调度策略计算出的优先级，进程分叉(fork)时，子进程会继承父进程的普通优先级
rt_priority | 用于保存实时优先级

优先级范围 | 描述
---|---
0——99 | 实时进程
100——139 | 非实时进程

- **调度策略**

```
// 进程的调度策略
unsigned int policy;

// 调度类，调度类，调度处理函数类
const struct sched_class *sched_class;
// 普通进程的调用实体，每个进程都有其中之一的实体
struct sched_entity se;
// 实时进程的调用实体，每个进程都有其中之一的实体
struct sched_rt_entity rt;
// deadline的调度实体
struct sched_dl_entity dl;

// 用于控制进程可以在哪里处理器上运行
cpumask_t cpus_allowed;
```

- **调度类**

```
struct sched_class {
    // 系统中多个调度类，按照其调度的优先级排成一个链表
    // 调度类优先级顺序: stop_sched_class -> dl_sched_class -> rt_sched_class -> fair_sched_class -> idle_sched_class
    const struct sched_class *next;

    // 向就绪队列中添加一个进程，它将调度实体（进程）放入红黑树中，并对 nr_running 变量加1
    void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);
    // 从运行队列中删除进程，它将从红黑树中去掉对应的调度实体，并对 nr_running 变量中减1
    void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);
    // 在进程想要资源放弃对处理器的控制权的时，在 compat_yield sysctl 关闭的情况下，该函数实际上执行先出队后入队，在这种情况下，它将调度实体放在红黑树的最右端  */
    void (*yield_task) (struct rq *rq);
    bool (*yield_to_task) (struct rq *rq, struct task_struct *p, bool preempt);
    // 检查当前进程是否可被新进程抢占
    void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int flags);


     // 选择下一个应该要运行的进程运行
    struct task_struct * (*pick_next_task) (struct rq *rq, struct task_struct *prev);
    // 将进程放回运行队列
    void (*put_prev_task) (struct rq *rq, struct task_struct *p);

#ifdef CONFIG_SMP
    // 为进程选择一个合适的CPU
    int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
    // 迁移任务到另一个CPU
    void (*migrate_task_rq)(struct task_struct *p);
    // 用于进程唤醒
    void (*task_waking) (struct task_struct *task);
    void (*task_woken) (struct rq *this_rq, struct task_struct *task);
    // 修改进程的CPU亲和力(affinity)
    void (*set_cpus_allowed)(struct task_struct *p,
                 const struct cpumask *newmask);
    // 启动运行队列
    void (*rq_online)(struct rq *rq);
     // 禁止运行队列
    void (*rq_offline)(struct rq *rq);
#endif
    // 当进程改变它的调度类或进程组时被调用
    void (*set_curr_task) (struct rq *rq);
    // 该函数通常调用自 time tick 函数，它可能引起进程切换，这将驱动运行时（running）抢占
    void (*task_tick) (struct rq *rq, struct task_struct *p, int queued);
    // 在进程创建时调用，不同调度策略的进程初始化不一样
    void (*task_fork) (struct task_struct *p);
    // 在进程退出时会使用
    void (*task_dead) (struct task_struct *p);


    // 用于进程切换
    void (*switched_from) (struct rq *this_rq, struct task_struct *task);
    void (*switched_to) (struct rq *this_rq, struct task_struct *task);
    // 改变优先级
    void (*prio_changed) (struct rq *this_rq, struct task_struct *task, int oldprio);

    unsigned int (*get_rr_interval) (struct rq *rq, struct task_struct *task);

    void (*update_curr) (struct rq *rq);

#ifdef CONFIG_FAIR_GROUP_SCHED
    void (*task_move_group) (struct task_struct *p);
#endif
};
```










---
> [Linux进程管理与调度-之-目录导航](https://blog.csdn.net/gatieme/article/details/51456569)
