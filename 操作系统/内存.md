# 内存

---
## Linux 内存概述

- **UMA和NUMA两种模型**
1. 均匀存储器存取（Uniform-Memory-Access，简称UMA）模型：传统的多核运算是使用SMP(Symmetric Multi-Processor )模式：将多个处理器与一个集中的存储器和I/O总线相连，所有处理器只能访问同一个物理存储器
2. 非均匀存储器存取（Nonuniform-Memory-Access，简称NUMA）模型：处理器可以同时访问不同的存储器地址，大幅度提高并行性，系统的哪个CPU都有本地内存，可支持快速的访问，各个处理器之前通过总线链接起来，以支持堆其他CPU的本地内存的访问，当然访问要比本地内存慢
3. NUMA模式下，处理器被划分成多个”节点”（node），每个节点被分配有的本地存储器空间，所有节点中的处理器都可以访问全部的系统物理存储器，但是访问本节点内的存储器所需要的时间，比访问某些远程节点内的存储器所花的时间要少得多


---
## 地址空间分布

- **x86的物理地址空间布局**
1. Linux内核将所有的物理页面划分到3类内存管理区
2. ZONE_DMA的范围是0~16M，该区域的物理页面专门供I/O设备的DMA使用，即DMA允许外围设备和主内存之间直接传输 I/O 数据，之所以需要单独管理DMA的物理页面，是因为DMA使用物理地址访问内存，不经过MMU，并且需要连续的缓冲区，所以为了能够提供物理上连续的缓冲区，必须从物理地址空间专门划分一段区域用于DMA
3. ZONE_NORMAL的范围是16M~896M，该区域的物理页面是内核能够直接使用的
4. ZONE_HIGHMEM的范围是896M~4G，该区域即为高端内存，内核不能直接使用

- **linux虚拟地址内核空间分布**
1. ZONE_DMA的范围是3G~3G+16M，该内核空间用于DMA操作
2. ZONE_NORMAL的范围是3G+16M～3G+896M，和内核线性空间存在直接映射关系，所以内核会将频繁使用的数据如kernel代码、GDT、IDT、PGD（页全局目录）、mem_map数组（页的数据结构对象数组）等放在ZONE_NORMAL里
3. ZONE_ HIGHMEM的范围是3G+896M～4G，位于内核空间高端的128M由3部分组成，分别为vmalloc area，持久化内核映射区，临时内核映射区，内核将用户数据、页表(PT)等不常用数据放在ZONE_HIGHMEM里，只在要访问这些数据时才建立映射关系(kmap())，比如，当内核要访问I/O设备存储空间时，就使用ioremap()将位于物理地址高端的mmio区内存映射到内核空间的vmalloc area中，在使用完之后便断开映射关系

- **linux虚拟地址用户空间分布**
1. 用户进程的代码区一般从虚拟地址空间的0x08048000开始，这是为了便于检查空指针
2. 数据区data
3. 未初始化数据区bss
4. 堆区heap
5. 栈区map
6. 参数、全局环境变量

- **linux虚拟地址与物理地址映射的关系**
1. 内核将0~896M的物理地址空间一对一映射到自己的线性地址空间中，这样它便可以随时访问ZONE_DMA和ZONE_NORMAL里的物理页面，此时内核剩下的128M线性地址空间不足以完全映射所有的ZONE_HIGHMEM，Linux采取了动态映射的方法，即按需的将ZONE_HIGHMEM里的物理页面映射到kernel space的最后128M线性地址空间里，使用完之后释放映射关系，以供其它物理页面映射

> [地址空间分布](https://www.cnblogs.com/chengxuyuancc/archive/2013/04/17/3026920.html)

---
## 用户内存布局

- **栈**
1. 调用一个方法或函数会将一个新的栈桢（stack frame）压入栈中，栈桢在函数返回时被清理，进程中的每一个线程都有属于自己的栈
2. 通过不断向栈中压入的数据，超出其容量就有会耗尽栈所对应的内存区域，这将触发一个页故障（page fault），并被Linux的expand_stack()处理，它会调用acct_stack_growth()来检查是否还有合适的地方用于栈的增长，如果栈的大小低于RLIMIT_STACK（通常是8MB），那么一般情况下栈会被加长，然而，如果达到了最大的栈空间大小，就会栈溢出（stack overflow），程序收到一个段错误（Segmentation Fault），当映射了的栈区域扩展到所需的大小后，它就不会再收缩回去，因此栈总是在增长的
3. 栈帧：保存被调函数返回时下一条执行指令的指针、主调函数的堆栈帧的指针、主调函数传递给被调函数的实参(如果有的话)、被调函数的局部变量等信息的一个结构
4. 如何区分每个堆栈帧：和栈密切相关的有2个寄存器，一个是ebp栈基址指针，一个是esp栈顶指针，esp是随着push和pop而不断移动

- **内存映射段**
1. 在栈的下方，是我们的内存映射段，内核将文件的内容直接映射到内存映射段，任何应用程序都可以通过Linux的mmap()系统调用请求这种映射，内存映射是一种方便高效的文件I/O方式，所以它被用于加载动态库
2. 在Linux中，如果你通过malloc()请求一大块内存，C运行库将会创建这样一个匿名映射而不是使用堆内存，"大块"意味着比MMAP_THRESHOLD还大，缺省是128KB

- **堆**
1. 堆用于存储那些生存期与函数调用无关的数据，大部分语言都提供了堆管理功能
2. 堆管理是很复杂的，需要精细的算法，应付我们程序中杂乱的分配模式，优化速度和内存使用效率

- **BSS 数据段 代码段**
1. BSS和数据段保存的都是静态（全局）变量的内容

> [linux系统进程的内存布局](https://www.cnblogs.com/diegodu/p/4552490.html)


---
## Linux 高端内存

- **Linux内核高端内存的划分与映射**
1. VMALLOC_START~VMALLOC_END：非连续内存分配   
(1)通过vmalloc()用来分配在虚拟地址空间连续，但是在物理地址空间不一定连续的内存区域，在"内核动态映射空间"申请内存的时候，就可能从高端内存获得页面
2. KMAP_BASE~FIXADDR_START：永久内核映射   
(1)高端页框到内核地址空间的长期映射   
(2)为了记录高端内存页框与永久内核映射的线性地址之间的联系，内核使用了page_address_htable散列表，该表包含一个page_address_map数据结构，用于为高端内存的每个页框进行当前映射，而该数据结构还包涵一个指向页描述符号的指针和分配给该页框的线性地址   
(3)通过 alloc_page() 获得了高端内存对应的 page，通常情况下，这个空间是 4M 大小，因此仅仅需要一个页表即可，内核通过来 pkmap_page_table 寻找这个页表，通过 kmap()，可以把一个 page 映射到这个空间来，由于这个空间是 4M 大小，最多能同时映射 1024 个 page，因此，对于不使用的的 page，应该及时通过 kunmap() 解除映射关系   
(4)page_address()函数返回页框对应的线性地址，如果页框在高端内存中并没有被映射，则返回NULL，这个函数接受一个页描述符指针page作为参数
3. FIXADDR_START~4G：临时内核映射   
(1)临时内核映射可以用在中断处理程序和可延迟函数的内部（这些函数不能被阻塞），建立临时内核映射禁用内核抢占   
(2)保留了一些线性空间用于特殊需求，每个 CPU 占用一块空间，在每个 CPU 占用的那块空间中，又分为多个小空间，每个小空间大小是 1 个 page，每个小空间用于一个目的，这些目的定义在 kmap_types.h 中的 km_type 中，通过 kmap_atomic() 可实现临时映射

- **内核高端内存的映射**
1. 当内核想访问高于896MB物理地址内存时，从0xF8000000 ~ 0xFFFFFFFF地址空间范围内找一段相应大小空闲的逻辑地址空间，借用这段逻辑地址空间，建立映射到想访问的那段物理内存（即填充内核页表项PTE），用完后释放

- **3G进程用户空间，1G为所有进程共享的内核空间**
1. 每个进程都有其自身的页面目录PGD（页全局目录），Linux将该目录的指针存放在与进程对应的内存结构task_struct.(struct mm_struct)mm->pgd中，每当一个进程被调度（schedule()）即将进入运行态时，Linux内核都要用该进程的PGD指针设置CR3（switch_mm()）
2. 当创建一个新的进程时，都要为新进程创建一个新的页面目录PGD，并从内核的页面目录swapper_pg_dir中复制内核区间页面目录项至新建进程页面目录PGD的相应位置（PGD在ZONE_NORMAL中），具体过程如下：
do_fork() --> copy_mm() --> mm_init() --> pgd_alloc() --> set_pgd_fast() --> get_pgd_slow() --> memcpy(&PGD + USER_PTRS_PER_PGD, swapper_pg_dir + USER_PTRS_PER_PGD, (PTRS_PER_PGD - USER_PTRS_PER_PGD) * sizeof(pgd_t))

> [linux 用户空间与内核空间——高端内存详解](https://blog.csdn.net/tommy_wxie/article/details/17122923/)   
> [Linux内存描述之高端内存](https://blog.csdn.net/gatieme/article/details/52384791)   
> [linux 高端内存页框管理：永久内核映射、临时内核映射以及非连续内存分配](https://blog.csdn.net/trochiluses/article/details/13016023)


---
## 页式管理

- **分页作用**
1. 给每一个进程分配一块不同的物理地址空间，这确保了可以有效地防止寻址错误
2. 区别页（即一组数据）和页框（即主存中的物理地址）之不同，这就允许存放在某个页框中的一个页，然后保存到磁盘上，以后重新装入这同一页时又被装在不同的页框中，这就是虚拟内存机制的基本要素

- **硬件分页支持**
1. 页：线性地址被分为以固定长度为单位的组，称为页，页内部连续的线性地址空间被映射到连续的物理地址中，这样，内核可以指定一个页的物理地址和对应的存取权限，而不用指定全部线性地址的存取权限
2. 页框：分页单元把所有的 RAM 分成固定长度的页框(page frame)(有时叫做物理页)，区分一页和一个页框是很重要的，前者只是一个数据块，可以存放在任何页框或磁盘中
3. 页表：把线性地址映射到物理地址的数据结构称为页表(page table)，页表存放在主存中

- **多级分页**
1. 多级页表目的在于减少每个进程页表所需的 RAM 的数量，每个活动的进程必须有一个页目录，但是却没有必要马上为所有进程的所有页表都分配 RAM，只有在实际需要一个页表时候才给该页表分配 RAM
2. 控制寄存器CR3存放正在使用的页目录的物理地址

- **四级分页机制**
1. 页全局目录（Page Global Directory）
2. 页上级目录（Page Upper Directory）
3. 页中间目录（Page Middle Directory）
4. 页表（Page Table）

- **地址转换过程**
1. 每次转换先获取物理页基地址，再从线性地址中获取索引，合成物理地址后再访问内存
2. 不管是页表还是要访问的数据都是以页为单位存放在主存中的，因此每次访问内存时都要先获得基址，再通过索引(或偏移)在页内访问数据，因此可以将线性地址看作是若干个索引的集合


> [深入理解计算机系统之内存寻址（五）--页式存储管理](https://blog.csdn.net/gatieme/article/details/50651561)   
> [深入理解计算机系统之内存寻址（六）--linux中的分页机制](https://blog.csdn.net/gatieme/article/details/50756050)   
> [Linux分页机制之概述](https://blog.csdn.net/gatieme/article/details/52402861)   
> [Linux x86_64线性地址空间布局](https://blog.csdn.net/junmuzi/article/details/18056115)

---
## 伙伴系统

- **伙伴关系**
1. 在操作系统分配内存的过程中，一个内存块常常被分成两个大小相等的内存块，这两个大小相等的内存块就处于伙伴关系，它满足 3 个条件：   
(1)两个块具有相同大小记为 2^K   
(2)它们的物理地址是连续的   
(3)从同一个大块中拆分出来

- **伙伴算法原理**
1. 为了便于页面的维护，将多个页面组成内存块，每个内存块都有 2 的方幂个页，方幂的指数被称为阶 order，order相同的内存块被组织到一个空闲链表中，伙伴系统基于2的方幂来申请释放内存页
2. 当申请内存页时，伙伴系统首先检查与申请大小相同的内存块链表中，检看是否有空闲页，如果有就将其分配出去，并将其从链表中删除
3. 否则就检查上一级，即大小为申请大小的2倍的内存块空闲链表，如果该链表有空闲内存，就将其分配出去，同时将剩余的一部分（即未分配出去的一半）加入到下一级空闲链表中
4. 如果这一级仍没有空闲内存，就检查它的上一级，依次类推，直到分配成功或者彻底失败，在成功时还要按照伙伴系统的要求，将未分配的内存块进行划分并加入到相应的空闲内存块链表
5. 在释放内存页时，会检查其伙伴是否也是空闲的，如果是就将它和它的伙伴合并为更大的空闲内存块，该检查会递归进行，直到发现伙伴正在被使用或者已经合并成了最大的内存块

- **linux中的伙伴系统相关的结构**

```
// 一次分配可以请求的页数最大是2^11=2048
# define MAX_ORDER 11

struct zone
{
    // 指定对应链表中的连续内存区包含多少个页帧
    struct free_area        free_area[MAX_ORDER];
};

struct free_area {
    // 用于将具有该大小的内存页块连接起来
    // 由于内存页块表示的是连续的物理页，因而对于加入到链表中的每个内存页块来说，只需要将内存页块中的第一个页加入该链表即可
    struct list_head        free_list[MIGRATE_TYPES];
    // 指定了当前内存页块的数目，对于0阶的表示以1页为单位计算，对于1阶的以2页为单位计算，n阶的以2的n次方为单位计算
    unsigned long           nr_free;
};
```

1. 内核中很多时候要求分配连续页，为快速检测内存中的连续区域，内核采用了一种古老而历经检验的技术：伙伴系统
2. 系统中的空闲内存块总是两两分组，每组中的两个内存块称作伙伴，伙伴的分配可以是彼此独立的，但如果两个伙伴都是空闲的，内核会将其合并为一个更大的内存块，作为下一层次上某个内存块的伙伴
3. 在应用程序释放内存时，内核可以直接检查地址，来判断是否能够创建一组伙伴，并合并为一个更大的内存块放回到伙伴列表中，这刚好是内存块分裂的逆过程，这提高了较大内存块可用的可能性


> [linux内核内存管理学习之二（物理内存管理--伙伴系统）](https://blog.csdn.net/goodluckwhh/article/details/9989695)   
> [伙伴系统之伙伴系统概述](https://blog.csdn.net/gatieme/article/details/52420444)


---
## slab 分配器

- **内核内存分配**
1. 在linux内核中伙伴系统用来管理物理内存，其分配的单位是页，内核也需要动态分配内存，而伙伴系统分配的粒度又太大，由于内核无法借助标准的C库，因而采用slab分配器
2. slab分配器不仅可以提供动态内存的管理功能，而且可以作为经常分配并释放的内存的缓存，通过slab缓存，内核能够储备一些对象，供后续使用
3. slab分配器分配的优点：   
(1)可以提供小块内存的分配支持   
(2)不必每次申请释放都和伙伴系统打交道，提供了分配释放效率   
(3)如果在slab缓存的话，其在CPU高速缓存的概率也会较高   
(4)slab分配器通过着色使得slab对象能够均匀的使用高速缓存，提高高速缓存的利用率

- **cache line 映射**
1. 如果一个地址可被映射进任意cache line，当寻找一个地址是否已经被cache时，需要遍历每一个cache line来寻找，这个代价不可接受
2. Direct Mapped Cache：相当于hash了，每个地址能被映射到的cache line是固定的，比如cache line有128(2^7)个，cache line的大小是32(2^5)字节，那么一个32位地址的 0~4位作为cache line内部偏移，5~11位作为cache line的索引即可，剩下的bit12~31作为当前cache line的 tag

- **cache line 字节对齐**
1. 数据跨越两个cache line，就意味着两次load或者两次store，如果数据结构是cache line对齐的，就有可能减少一次读写，数据结构的首地址cache line对齐，意味着可能有内存浪费（特别是数组这样连续分配的数据结构），所以需要在空间和时间两方面权衡

- **slab着色与cpu硬件高速缓存**
1. 一个slab大小肯定是整数页，所以起始地址末12位为零，如果2个slab的每一个obj大小一样，所以2个slab每个obj都会对应相同的cache line，当进行针对这两个对象的读操作时，就出现了两个对象在cache line和RAM之间来回不停切换的现象
2. 由于slab是采用空间换时间的方式提高分配效率，因此在slab块中会存在没有用处的字节，无用字节被划分为着色域和着色补偿域以及对齐因子，着色域后面紧跟着slab描述符+对象描述符，接下来就是每个对象了，slab尾部紧跟着着色补偿域，这样，就使得对象大小相同的不同slab块中的对象拥有不同的位移量，确保它们不会被映射到硬件缓存内部相同的cache line中，尽量避免cache line冲突

- **slab 结构**

```
// 缓存 kmem_cache
struct kmem_cache {
    struct array_cache *array[NR_CPUS];
    unsignedint batchcount; // 从本地高速缓存交换的对象的数量
    unsignedint limit;      // 本地高速缓存中空闲对象的数量
    unsignedint shared;     // 是否存在共享CPU高速缓存
    unsigned int buffer_size;   // 对象长度+填充字节
    // 组织该高速缓存中的slab
    structkmem_list3 *nodelists[MAX_NUMNODES];/
}

// array_cache 本地高速缓存，每个CPU对应一个该结构
struct array_cache {
    unsigned int avail;         // 可用对象数目
    unsigned int limit;         // 可拥有的最大对象数目，和kmem_cache中一样
    unsigned int batchcount;    // 同kmem_cache
    unsigned int touched;       // 是否在收缩后被访问过
    spinlock_t lock;
    void *entry[];              // 伪数组，没有任何数据项，其后为释放的对象指针数组
};

// 管理 slab 链表的数据结构，分为 full，partial，free
struct kmem_list3 {
    struct list_head slabs_partial;
    struct list_head slabs_full;
    struct list_head slabs_free;
    unsigned long free_objects;     // 半空和全空链表中对象的个数
    unsigned int free_limit;        // 所有slab上允许未使用的对象最大数目
    unsigned int colour_next;       // 下一个slab的颜色
    spinlock_t list_lock;
    struct array_cache *shared;     // shared per node
    struct array_cache **alien;     // on other nodes
    unsigned long next_reap;        // 两次缓存收缩时的间隔，降低次数，提高性能
    int free_touched;               // 0收缩1获取一个对象
};

// slab 对象
struct slab {
    struct list_head list;      // SLAB所在的链表
    unsigned long colouroff;    // SLAB中第一个对象的偏移
    void *s_mem;                // 第一个对象的地址
    unsigned int inuse;         // slab已使用的对象数目
    kmem_bufctl_t free;         // 下一个空闲对象的下标
    unsigned short nodeid;      // 用于寻址在高速缓存中kmem_list3的下标
};
```

> [linux内核内存管理学习之三（slab分配器）](https://blog.csdn.net/goodluckwhh/article/details/10026311)

> [slab着色](https://blog.csdn.net/zqy2000zqy/article/details/1137895)   
> [slab着色与cpu硬件高速缓存](http://www.360doc.com/content/12/0828/15/7982302_232808690.shtml)   
> [cpu cache line 原理](https://blog.csdn.net/zdl1016/article/details/8882092)   
> [CPU cache 与内存对齐](https://blog.csdn.net/zhang_shuai_2011/article/details/38119657)

> [Linux内存管理之slab机制](https://blog.csdn.net/bullbat/article/details/7194794)   
> [SLAB原理及实现](https://blog.csdn.net/chenxiancool/article/details/7638804)


---
## malloc 底层实现原理

- **malloc 堆分配**
1. 分配的是虚拟内存，没有分配物理内存，在第一次访问已分配的虚拟地址空间的时候，发生缺页中断，操作系统负责分配物理内存，然后建立虚拟内存和物理内存之间的映射关系
2. Linux维护一个break指针，这个指针指向堆空间的某个地址，从堆起始地址到break之间的地址空间为映射好的，可以供进程访问，而从break往上，是未映射的地址空间，如果访问这段空间则程序会报错，我们用malloc进行内存分配就是从break往上进行的
3. malloc 函数的实质是它有一个将可用的内存块连接为一个长长的列表的空闲链表，调用malloc（）函数时，它沿着连接表寻找一个大到足以满足用户请求所需要的内存块，然后，将该内存块一分为二（一块的大小与用户申请的大小相等，另一块的大小就是剩下来的字节），接下来，将分配给用户的那块内存存储区域传给用户，并将剩下的那块（如果有的话）返回到连接表上
4. 调用 free 函数时，它将用户释放的内存块连接到空闲链表上
5. 到最后，空闲链会被切成很多的小内存片段，如果这时用户申请一个大的内存片段，那么空闲链表上可能没有可以满足用户要求的片段了，于是 malloc（）函数请求延时，并开始在空闲链表上检查各内存片段，对它们进行内存整理，将相邻的小空闲块合并成较大的内存块

- **malloc 结构**

```
// 内存控制块
struct mem_control_block
{	
	int is_available;   // 是否空闲
	int size;           // 内存块大小
};

// 所要申请的内存是由多个内存块构成的链表
typedef struct s_block *t_block;
struct s_block {
    size_t size;    // 数据区大小
    t_block next;   // 指向下个块的指针
    int free;       // 是否是空闲块
    int padding;    // 填充4字节，保证meta块长度为8的倍数
    char data[1]    // 这是一个虚拟字段，表示数据块的第一个字节，长度不应计入meta
};

// 被映射的内存边界（操作系统最后一个有效地址）常被称为系统中断点或者当前中断点，为了指出当前系统中断点，必须使用 sbrk() 函数
// sbrk 函数根据参数中给出的字节数将break从当前位置移动increment所指定的增量，sbrk成功时返回break移动之前所指向的地址
// sbrk(0) 只是返回当前中断点
void malloc_init()
{
    last_valid_address = sbrk(0);   // 用 sbrk 函数在操作系统中取得最后一个有效地址
    managed_memory_start = last_valid_address;  // 将最后一个有效地址作为管理内存的起始地址
    has_initialized = 1;                        // 初始化成功标记
}

// 寻找合适的 block
// First fit：从头开始，使用第一个数据区大小大于要求size的块所谓此次分配的块（更好的运行效率）
// Best fit：从头开始，遍历所有块，使用数据区大小大于size且差值最小的块作为此次分配的块（较高的内存使用率）

// 如果现有block都不能满足size的要求，则需要在链表最后开辟一个新的block，调用sbrk()创建新的block
sbrk(numbytes)

// free 要释放的内存块首地址
void free(void *firstbyte)
{
    struct mem_control_block *mcb;
    //取得该块的内存控制块的首地址
    mcb = firstbyte - sizeof(struct mem_control_block);
    //将该块标志设为可用
    mcb->is_available = 1;
    return;
}
```

> [linux-malloc底层实现原理](https://blog.csdn.net/mmshixing/article/details/51679571)    
> [linux下malloc()和free()的原理及实现](https://blog.csdn.net/c1s2p3/article/details/50522185)  
> [malloc 底层实现及原理](https://www.cnblogs.com/zpcoding/p/10808969.html)   
> [brk/sbrk工作原理](https://blog.csdn.net/aspnet_lyc/article/details/20801957)


---
## 内存映射文件 mmap

- **内存映射原理**
1. 硬盘上文件的位置与进程逻辑地址空间（堆和栈中间，Memory Mapping Segment 内存映射区）中一块大小相同的区域之间的一一对应，这种对应关系纯属是逻辑上的概念，物理上是不存在的，在内存映射的过程中，并没有实际的数据拷贝，文件没有被载入内存，只是逻辑上被放入了内存，具体到代码，就是建立并初始化了相关的数据结构（struct address_space），这个过程有系统调用mmap()实现，所以建立内存映射的效率很高
2. 进程怎么能最终直接通过内存操作访问到硬盘上的文件？   
(1)mmap()会返回一个指针ptr，它指向进程逻辑地址空间中的一个地址，进程无需再调用read或write对文件进行读写，而只需要通过ptr就能够操作文件，但是ptr所指向的是一个逻辑地址，要操作其中的数据，必须通过MMU将逻辑地址转换成物理地址，这个过程与内存映射无关   
(2)建立内存映射并没有实际拷贝数据，MMU在地址映射表中是无法找到与ptr相对应的物理地址的，也就是MMU失败，将产生一个缺页中断，缺页中断的中断响应函数会在swap中寻找相对应的页面，如果找不到（也就是该文件从来没有被读入内存的情况），则会通过mmap()建立的映射关系，从硬盘上将文件读取到物理内存中，这个过程与内存映射无关   
(3)如果在拷贝数据时，发现物理内存不够用，则会通过虚拟内存机制（swap）将暂时不用的物理页面交换到硬盘上)，这个过程与内存映射无关

- **效率**
1. read()是系统调用，其中进行了数据拷贝，它首先将文件内容从硬盘拷贝到内核空间的一个缓冲区，然后再将这些数据拷贝到用户空间，在这个过程中，实际上完成了两次数据拷贝
2. mmap()中没有进行数据拷贝，真正的数据拷贝是在缺页中断处理时进行的，由于mmap()将文件直接映射到用户空间，所以中断处理函数根据这个映射关系，直接将文件从硬盘拷贝到用户空间，只进行了 一次数据拷贝 

> [内存映射文件原理探索](https://blog.csdn.net/mg0832058/article/details/5890688)












---
> [深入理解Linux内存管理之目录导航](https://blog.csdn.net/gatieme/article/details/52384965)   
> [理解Linux内存管理](https://blog.csdn.net/gatieme/category_6393814.html)